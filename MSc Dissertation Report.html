<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><link rel="stylesheet" href="MSc Dissertation Report_files/MSc Dissertation Report.css" type="text/css"/>
</head>
<body>
<p><span class="font5">Imperial College London Technology and Medicine</span></p>
<p><span class="font5">(University of London) Department of Computing</span></p><h2><a name="caption1"></a><a name="bookmark0"></a><span class="font8">Reading Sheet Music Arnaud F. Desaedeleer (afd05)</span></h2>
<p><span class="font33">Submitted in partial fulfilment of the requirements for the MSc&nbsp;Degree in Advanced Computing of the&nbsp;University of London and for the&nbsp;Diploma of Imperial College of&nbsp;Science, Technology and Medicine.</span></p>
<p><span class="font6">September 2006</span></p>
<p><span class="font33" style="font-weight:bold;">Abstract</span></p>
<p><span class="font33">Optical Music Recognition is the process of recognising a printed music score and converting it to a format that is understood by computers. This process&nbsp;involves detecting all musical elements present in the music score in such a way&nbsp;that the score can be represented digitally. For example, the score could be&nbsp;recognised and played back through the computer speakers. Much research has&nbsp;been carried out in this area and several approaches to performing OMR have&nbsp;been suggested. A more recent approach involves segmenting the image using&nbsp;a neural network to recognise the segmented symbols from which the score can&nbsp;be reconstructed. This project will survey the different techniques that have&nbsp;been used to perform OMR on printed music scores and an application by the&nbsp;name of OpenOMR will be developed. One of the aims is to create an open&nbsp;source project in which developers in the open source community will be able&nbsp;to contribute their ideas in order to enhance this application and progress the&nbsp;research in the OMR field.</span></p><h1><a name="caption2"></a><a name="bookmark1"></a><span class="font16">Acknowledgements</span></h1>
<p><span class="font33">I would like to thank Dr. Simon Colton for his help and support throughout&nbsp;this project. He made the time to meet with me on a regular basis and provided&nbsp;me with feedback which contributed to the success of this pro ject.</span></p>
<p><span class="font33">I would also like to thank my parents and my sister for all their support. Without them, I would not be here right now.</span></p><h1><a name="bookmark2"></a><span class="font16">Contents</span></h1>
<p><span class="font33" style="font-weight:bold;">1 Introduction 5</span></p>
<p><span class="font33"><a href="#bookmark3">1.1 &nbsp;&nbsp;&nbsp;Motivation .............................. 5</a></span></p>
<p><span class="font33"><a href="#bookmark4">1.2 &nbsp;&nbsp;&nbsp;Report Outline ............................ 6</a></span></p>
<p><span class="font33"><a href="#bookmark5">1.3 &nbsp;&nbsp;&nbsp;OpenOMR .............................. 6</a></span></p>
<p><span class="font33" style="font-weight:bold;"><a href="#bookmark6">2 Background&nbsp;&nbsp;&nbsp;&nbsp;7</a></span></p>
<p><span class="font33"><a href="#bookmark7">2.1 &nbsp;&nbsp;&nbsp;Music Theory and Terminology .................. 7</a></span></p>
<p><span class="font33"><a href="#bookmark8">2.2 &nbsp;&nbsp;&nbsp;Music Typesetting ......................... 8</a></span></p>
<p><span class="font33"><a href="#bookmark9">2.3 &nbsp;&nbsp;&nbsp;Commercial OMR Systems ..................... 9</a></span></p>
<p><span class="font33"><a href="#bookmark10">2.4 &nbsp;&nbsp;&nbsp;Music Notation File Formats ................... 10</a></span></p>
<p><span class="font33"><a href="#bookmark11">2.5 &nbsp;&nbsp;&nbsp;Evaluation of OMR Systems ..................... 11</a></span></p>
<p><span class="font33"><a href="#bookmark12">2.6 &nbsp;&nbsp;&nbsp;A Typical OMR Process ...................... 13</a></span></p>
<p><span class="font33"><a href="#bookmark13">2.7 &nbsp;&nbsp;&nbsp;Issues/Problems Associated with OMR Systems ......... 13</a></span></p>
<p><span class="font33"><a href="#bookmark14">2.7.1 &nbsp;&nbsp;&nbsp;Graphic Quality and Print&nbsp;&nbsp;&nbsp;&nbsp;Faults ............. 13</a></span></p>
<p><span class="font33"><a href="#bookmark15">2.7.2 &nbsp;&nbsp;&nbsp;Handwritten Partitions ................... 15</a></span></p>
<p><span class="font33"><a href="#bookmark16">2.8 &nbsp;&nbsp;&nbsp;OMR Research/Projects ...................... 15</a></span></p>
<p><span class="font33"><a href="#bookmark17">2.8.1 &nbsp;&nbsp;&nbsp;Carter (1988) ........................ 15</a></span></p>
<p><span class="font33"><a href="#bookmark18">2.8.2 &nbsp;&nbsp;&nbsp;Fujinaga (1988) ....................... 15</a></span></p>
<p><span class="font33"><a href="#bookmark19">2.8.3 &nbsp;&nbsp;&nbsp;Roth (1994) ......................... 16</a></span></p>
<p><span class="font33"><a href="#bookmark20">2.8.4 &nbsp;&nbsp;&nbsp;Bainbridge (1991-1997)&nbsp;&nbsp;&nbsp;&nbsp;  17</a></span></p>
<p><span class="font33"><a href="#bookmark21">2.8.5 &nbsp;&nbsp;&nbsp;Ng and Boyle (1992-2002)................. 17</a></span></p>
<p><span class="font33"><a href="#bookmark22">2.8.6 &nbsp;&nbsp;&nbsp;The O</span><span class="font29"><sup>3</span><span class="font33"></sup>MR ......................... 18</span></a></p>
<p><span class="font33"><a href="#bookmark23">2.9 &nbsp;&nbsp;&nbsp;Optical&nbsp;&nbsp;&nbsp;&nbsp;Music Recognition using Kd-tree Decomposition ..... 18</a></span></p>
<p><span class="font33"><a href="#bookmark24">2.10 &nbsp;&nbsp;&nbsp;Music Sheet Reader ......................... 19</a></span></p>
<p><span class="font33"><a href="#bookmark25">2.11 &nbsp;&nbsp;&nbsp;Run Length Encoding&nbsp;&nbsp;&nbsp;&nbsp;........................ 19</a></span></p>
<p><span class="font33"><a href="#bookmark26">2.12 &nbsp;&nbsp;&nbsp;X- and Y- Projection Definition .................. 20</a></span></p>
<p><span class="font33"><a href="#bookmark27">2.13 &nbsp;&nbsp;&nbsp;Discrete Fourier Transform&nbsp;&nbsp;&nbsp;&nbsp;(DFT) ................. 20</a></span></p>
<p><span class="font33"><a href="#bookmark28">2.14 &nbsp;&nbsp;&nbsp;Artificial Neural Networks&nbsp;&nbsp;&nbsp;&nbsp;...................... 23</a></span></p>
<p><span class="font33"><a href="#bookmark29">2.14.1 &nbsp;&nbsp;&nbsp;Supervised and&nbsp;&nbsp;&nbsp;&nbsp;Unsupervised Training .......... 23</a></span></p>
<p><span class="font33"><a href="#bookmark30">2.14.2 &nbsp;&nbsp;&nbsp;The Artificial Neuron .................... 23</a></span></p>
<p><span class="font33"><a href="#bookmark31">2.14.3 &nbsp;&nbsp;&nbsp;Feedforward Neural&nbsp;&nbsp;&nbsp;&nbsp;Networks ................ 24</a></span></p>
<p><span class="font33"><a href="#bookmark32">2.14.4 &nbsp;&nbsp;&nbsp;Local Minima ........................ 26</a></span></p>
<p><span class="font33"><a href="#bookmark33">2.14.5 &nbsp;&nbsp;&nbsp;Overfitting ......................... 27</a></span></p>
<p><span class="font33"><a href="#bookmark34">2.14.6 &nbsp;&nbsp;&nbsp;Self Organising Feature Map (SOFM) ........... 27</a></span></p>
<p><span class="font33"><a href="#bookmark35">2.15 &nbsp;&nbsp;&nbsp;Software Implementations of Neural Networks .......... 28</a></span></p>
<p><span class="font33"><a href="#bookmark36">2.16 &nbsp;&nbsp;&nbsp;Other Software Used ......................... 28</a></span></p>
<p><span class="font33"><a href="#bookmark37">2.17 &nbsp;&nbsp;&nbsp;Summary ............................... 28</a></span></p>
<p><span class="font33" style="font-weight:bold;"><a href="#bookmark38">3 Design&nbsp;&nbsp;&nbsp;&nbsp;30</a></span></p>
<p><span class="font33"><a href="#bookmark39">3.1 Overview ............................... 30</a></span></p>
<p><span class="font33"><a href="#bookmark40">3.2 Programming Language Choice .................. 32</a></span></p>
<p><span class="font33"><a href="#bookmark41">3.3 Stave Detection ............................ 33</a></span></p>
<p><span class="font33"><a href="#bookmark42">3.3.1 Stave Line Parameter Detection .............. 33</a></span></p>
<p><span class="font33"><a href="#bookmark43">3.3.2 Stave Detection ........................ 34</a></span></p>
<p><span class="font33"><a href="#bookmark44">3.4 Skew Correction ........................... 36</a></span></p>
<p><span class="font33"><a href="#bookmark45">3.5 Image Segmentation (Level 0) .................... 37</a></span></p>
<p><span class="font33"><a href="#bookmark46">3.6 Note Head Detection ......................... 38</a></span></p>
<p><span class="font33"><a href="#bookmark47">3.7 Symbol Segmentation (Level 1) ................... 38</a></span></p>
<p><span class="font33"><a href="#bookmark48">3.8 Note Processing (Level 2) ...................... 39</a></span></p>
<p><span class="font33"><a href="#bookmark49">3.9 Neural Network ............................ 40</a></span></p>
<p><span class="font33">3.9.1 Neural Network Design Considerations and Choices . . . 41</span></p>
<p><span class="font33"><a href="#bookmark50">3.9.2 Normalising Image Segments ............... 41</a></span></p>
<p><span class="font33"><a href="#bookmark51">3.10 Midi Music Generation ....................... 42</a></span></p>
<p><span class="font33"><a href="#bookmark52">3.11 Summary .............................. 43</a></span></p>
<p><span class="font33" style="font-weight:bold;"><a href="#bookmark53">4 Implementation&nbsp;&nbsp;&nbsp;&nbsp;45</a></span></p>
<p><span class="font33"><a href="#bookmark54">4.1 Package Structure .......................... 45</a></span></p>
<p><span class="font33"><a href="#bookmark55">4.1.1 The openomr.ann Package ................. 45</a></span></p>
<p><span class="font33"><a href="#bookmark56">4.1.2 &nbsp;&nbsp;&nbsp;openomr.data_analysis.................... 48</a></span></p>
<p><span class="font33"><a href="#bookmark57">4.1.3 &nbsp;&nbsp;&nbsp;openomr .fft .......................... 49</a></span></p>
<p><span class="font33"><a href="#bookmark58">4.1.4 &nbsp;&nbsp;&nbsp;openomr .gui .......................... 49</a></span></p>
<p><span class="font33"><a href="#bookmark59">4.1.5 &nbsp;&nbsp;&nbsp;openomr.imageprocessing .................. 49</a></span></p>
<p><span class="font33"><a href="#bookmark60">4.1.6 &nbsp;&nbsp;&nbsp;openomr.midi ......................... 49</a></span></p>
<p><span class="font33"><a href="#bookmark61">4.1.7 &nbsp;&nbsp;&nbsp;openomr.omr .engine..................... 51</a></span></p>
<p><span class="font33"><a href="#bookmark62">4.2 &nbsp;&nbsp;&nbsp;Using the OpenOMR Engine .................... 55</a></span></p>
<p><span class="font33"><a href="#bookmark63">4.3 &nbsp;&nbsp;&nbsp;summary ............................... 56</a></span></p>
<p><span class="font33" style="font-weight:bold;"><a href="#bookmark64">5 Testing&nbsp;&nbsp;&nbsp;&nbsp;57</a></span></p>
<p><span class="font33"><a href="#bookmark65">5.1 &nbsp;&nbsp;&nbsp;Testing Environment ......................... 57</a></span></p>
<p><span class="font33"><a href="#bookmark66">5.2 &nbsp;&nbsp;&nbsp;False Positives and False Negatives ................. 57</a></span></p>
<p><span class="font33"><a href="#bookmark67">5.2.1 &nbsp;&nbsp;&nbsp;FFT Module ......................... 57</a></span></p>
<p><span class="font33"><a href="#bookmark68">5.2.2 &nbsp;&nbsp;&nbsp;Stave Detection ........................ 58</a></span></p>
<p><span class="font33"><a href="#bookmark69">5.2.3 &nbsp;&nbsp;&nbsp;Note Heads Detected ..................... 58</a></span></p>
<p><span class="font33"><a href="#bookmark70">5.2.4 &nbsp;&nbsp;&nbsp;Pitch Calculation ...................... 58</a></span></p>
<p><span class="font33"><a href="#bookmark71">5.2.5 &nbsp;&nbsp;&nbsp;Note Duration ....................... 58</a></span></p>
<p><span class="font33"><a href="#bookmark72">5.2.6 &nbsp;&nbsp;&nbsp;Neural Network ........................ 59</a></span></p>
<p><span class="font33"><a href="#bookmark73">5.3 &nbsp;&nbsp;&nbsp;Graphical User Interface Testing ................. 59</a></span></p>
<p><span class="font33"><a href="#bookmark74">5.3.1 &nbsp;&nbsp;&nbsp;Monkey Testing ....................... 59</a></span></p>
<p><span class="font33"><a href="#bookmark75">5.3.2 &nbsp;&nbsp;&nbsp;Stress Testing ........................ 59</a></span></p>
<p><span class="font33" style="font-weight:bold;">6 Results 60</span></p>
<p><span class="font33"><a href="#bookmark76">6.1 &nbsp;&nbsp;&nbsp;FFT Module ............................. 60</a></span></p>
<p><span class="font33"><a href="#bookmark77">6.2 &nbsp;&nbsp;&nbsp;Stave Detection ............................ 60</a></span></p>
<p><span class="font33"><a href="#bookmark78">6.3 &nbsp;&nbsp;&nbsp;Note Head Detected ......................... 61</a></span></p>
<p><span class="font33"><a href="#bookmark79">6.4 &nbsp;&nbsp;&nbsp;Pitch Calculation .......................... 61</a></span></p>
<p><span class="font33"><a href="#bookmark80">6.5 &nbsp;&nbsp;&nbsp;Note Duration ............................ 65</a></span></p>
<p><span class="font33"><a href="#bookmark81">6.6 &nbsp;&nbsp;&nbsp;Neural Network ............................ 65</a></span></p>
<p><span class="font33"><a href="#bookmark82">6.7 &nbsp;&nbsp;&nbsp;Monkey Testing ............................ 66</a></span></p>
<table border="1">
<tr><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font33">6.8 Stress Testing .................</span></p></td><td style="vertical-align:middle;">
<p><span class="font33">............ 66</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">7</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">Future Work</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">67</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">8</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">Conclusion</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">69</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">A</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">Scores</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">70</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">B</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">Musical Glyphs Used</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">72</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">C</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">OpemOMR User Guide</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">73</span></p></td></tr>
</table><h1><span class="font13">Chapter 1</span></h1><h1><a name="bookmark83"></a><span class="font16">Introduction</span></h1>
<p><span class="font33">This project will involve implementing a software toolkit that will be able to process a printed music score and transform it into a format understandable&nbsp;by the computer. This process is known as Optical Music Recognition (OMR)&nbsp;and has been an active research area since the 1970's. In contrast to Optical&nbsp;Character Recognition (OCR) systems, OMR systems are subject to greater&nbsp;complexities as music notation is represented in a two-dimensional space. The&nbsp;horizontal direction can be associated with the note duration (time) whilst the&nbsp;vertical direction can be associated with pitch.</span></p>
<p><span class="font33">There are several applications to OMR systems and we will now consider a few of them. Scores sometimes need to be adapted to different instruments&nbsp;(transposed) and having the score in a digital format greatly reduces the time&nbsp;and effort required to do that. Some scores are extremely old and it is convenient&nbsp;to archive them in a digital format. An OMR system is an ideal tool for archiving&nbsp;and re-printing old scores. Converting music scores in Braille code for the blind&nbsp;is yet another application of an OMR system. [10]</span></p><h2><a name="bookmark3"></a><span class="font8">1.1 Motivation</span></h2>
<p><span class="font33">There are several factors that led to the motivation of this project. The main motivation behind this pro ject was driven Dr. Simon Colton. Having an extensive collection of piano scores and not having the time to learn/play each score,&nbsp;he wanted an application that could play those for him. Having not found any&nbsp;satisfactory software available, he decided to turn it into a student project.</span></p>
<p><span class="font33">Having a strong interest for music and an interest for computer graphics, I decided to take on this pro ject. I was not able to find any free or open source&nbsp;OMR toolkits. The only available ones are expensive, and when a particular&nbsp;demonstration version was tested, it performed rather poorly. Having used many&nbsp;open source pro jects available on the Internet, I feel this is an opportunity for&nbsp;me to contribute to the open source community. Furthermore, I want to start an&nbsp;open source project for which passionate musicians and computer scientists can&nbsp;devote their time and effort in order to help improve this toolkit and advance&nbsp;in the research area of OMR.</span></p><h2><a name="bookmark4"></a><span class="font8">1.2 Report Outline</span></h2>
<p><span class="font33">The report for this project is organised as follows:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Background </span><span class="font33">- This chapter provides a description of related works in&nbsp;the field of OMR. Furthermore, the basic theory to be used for the design&nbsp;and implementation of the project is presented.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Design </span><span class="font33">- This chapter provides a detailed description of the different&nbsp;components of the OMR system that is implemented for this project.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Implementation </span><span class="font33">- In this chapter, a technical description of the various&nbsp;components and how they interact with each other is provided.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Testing </span><span class="font33">- This chapter discusses how the OMR application was tested&nbsp;along with all the parameters that were varied when the testing phase was&nbsp;conducted.</span></p>
<p><span class="font33">5. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Results </span><span class="font33">- The application was tested with several scores and the results&nbsp;in terms of accuracy are provided in this chapter.</span></p>
<p><span class="font33">6. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Future Improvements </span><span class="font33">- This chapter talks about the future development of the OMR application implemented in this project.</span></p><h2><a name="bookmark5"></a><span class="font8">1.3 OpenOMR</span></h2>
<p><span class="font33">This project is hosted on the Sourceforge website from which all the source code can be downloaded. The URL to the project home page is given below:</span></p>
<p><span class="font4"><a href="http://openomr.sourceforge.net">http://openomr.sourceforge.net</a></span></p><h1><span class="font13">Chapter 2</span></h1><h1><a name="bookmark6"></a><span class="font16">Background</span></h1>
<p><span class="font33">The goal of this chapter is to build the basic foundations needed in order to understand how an OMR system works along with the theory used for the&nbsp;implementation of the optical music recognition system. We first begin by briefly&nbsp;mentioning the underlying concepts of music theory after which two musical&nbsp;typesetting programs will be introduced. An evaluation of current commercial&nbsp;OMR systems is then presented, followed by a description of how OMR systems&nbsp;can be evaluated. We will then look at some issues and problems which make&nbsp;the OMR tricky. Previous research and projects are then summarised in order&nbsp;to gain a certain knowledge in the field of OMR. Finally, some underlying theory&nbsp;which is used in the design and implementation of the OMR system is presented.</span></p><h2><a name="bookmark7"></a><span class="font8">2.1 Music Theory and Terminology</span></h2>
<p><span class="font33">In order to understand how OMR systems work, it is important to have a notion of the underlying concepts in music theory and its associated terminology.</span></p>
<p><span class="font33">The fundamental music element in a music score is the stave (also referred to as the staff ). The stave is composed of five horizontal lines and four spaces&nbsp;in between each line. Vertical bars (known as a bar line) are placed in order to&nbsp;separate different measures. There are three types of clefs and they are known&nbsp;as the treble, bass and alto clefs. The clef is commonly placed at the beginning&nbsp;of the stave although it can also appear anywhere in a measure.</span></p>
<p><span class="font33">Polyphonic scores are ones in which multiple independent melodies are present as opposed to monophonic scores in which only one is present. Piano scores typically use a grand stave which is composed of a bass and treble stave.</span></p>
<p><span class="font33">Notes are placed either on a stave line or in between a stave line. When a note is placed above or below the stave, ledger lines are used. The higher the&nbsp;note's placement on the stave, the higher its pitch. Depending on the type of&nbsp;note, it will be played for a certain duration.</span></p>
<p><span class="font33">Figure 2.1 shows a series of notes (top) and rests (bottom). The number below each note and rest indicates their associated temporal value as highlighted&nbsp;in figure 2.1. The notion of time in musical scores is represented by a meter.&nbsp;The meter indicates how many notes of the same type are to be played in one&nbsp;measure and is expressed as a fraction. The numerator indicates how many&nbsp;beats occur in a measure and the denominator indicates how many beats a</span></p>
<p><span class="font14" style="text-decoration:line-through;">o &nbsp;&nbsp;&nbsp;J&nbsp;&nbsp;&nbsp;&nbsp;J&nbsp;&nbsp;&nbsp;&nbsp;&gt;&nbsp;&nbsp;&nbsp;&nbsp;J&nbsp;&nbsp;&nbsp;&nbsp;J&nbsp;&nbsp;&nbsp;&nbsp;)</span></p>
<p><span class="font28">1 &nbsp;&nbsp;&nbsp;1/2&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font28" style="font-weight:bold;">1/4&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font28">1/8&nbsp;&nbsp;&nbsp;&nbsp;1/16&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font30" style="font-weight:bold;">1/32&nbsp;&nbsp;&nbsp;&nbsp;1/64</span></p>
<p><span class="font29" style="font-weight:bold;"><sup>1</sup> &nbsp;&nbsp;&nbsp;1/2&nbsp;&nbsp;&nbsp;&nbsp;1/4&nbsp;&nbsp;&nbsp;&nbsp;1/8&nbsp;&nbsp;&nbsp;&nbsp;1/16&nbsp;&nbsp;&nbsp;&nbsp;1/32 Jy<sub>64</sub></span></p>
<p><span class="font33">Figure 2.1: Notes and Rests (images taken from [18])</span></p>
<p><span class="font33">crotchet receives. A </span><span class="font29">4 </span><span class="font33">beat is also referred to as a common beat and is usually represented by a C instead of a fraction.</span></p><h2><a name="bookmark8"></a><span class="font8">2.2 Music Typesetting</span></h2>
<p><span class="font33">There exist several music typesetting packages in the open source community of which two will be briefly mentioned. The output of the OMR system that will&nbsp;be implemented for this project should be able to interact with a typesetting&nbsp;package by means of a common notation format. This will be useful for debugging purposes but will also be useful for its users to view the processed score&nbsp;in a user friendly format. Figures 2.1 and 2.2 were typeset using the LilyPond&nbsp;package.</span></p>
<p><span class="font33">LilyPond is a program that enables musicians to mark up music scores by using a simple ASCII notation as its input. That notation is then processed by&nbsp;the LilyPond program and is able to output a music score in several different&nbsp;formats including the P ostScript and EP S formats. Furthermore, LilyPond&nbsp;is capable of reading different file formats including the M usicXML and the&nbsp;M I DI file format which are both described in section 2.4.</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font13">$</span></p></td><td colspan="3">
<p></p></td><td colspan="3">
<p></p></td><td colspan="2">
<p></p></td></tr>
<tr><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font3">r r r r</span></p></td><td colspan="2">
<p></p></td><td>
<p></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font6" style="font-style:italic;">p</span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font3">_</span></p></td><td>
<p></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font13">-</span></p></td><td>
<p><span class="font3">4-</span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p><span class="font3">—</span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td></tr>
</table>
<p><span class="font33">Figure 2.2: Example of a stave typeset with LilyPond</span></p>
<p><span class="font33">The LilyPond package is extremely well documented and many examples are provided, making it an attractive package to use. LilyPond also runs on many&nbsp;different platforms (Linux, Windows and Mac OS X) and does not require the&nbsp;installation of dependant packages. The installation of the LilyPond package is&nbsp;simple, making it a popular typesetting application used amongst professional&nbsp;musicians. [15]</span></p>
<p><span class="font33">For example, the code used to represent figure 2.2 looks as follows:</span></p>
<p><span class="font4">\relative c''</span></p>
<p><span class="font4">{</span></p>
<p><span class="font4">{</span></p>
<p><span class="font4">\time 4/4 \clef treble</span></p>
<p><span class="font4">a1 b2 c4 d8 d8 d8 d8 e16 e16 e16 e16 e16 e16 e16 s16_&quot; &quot;</span></p>
<p><span class="font4">}</span></p>
<p><span class="font4">}</span></p>
<p><span class="font33">MusiXT<sub>E</sub>X consists of a set of T<sub>E</sub>X macros enabling users to typeset music scores within T<sub>E</sub>X documents. It therefore requires the T<sub>E</sub>X package to be&nbsp;installed, and that the MusiXT<sub>E</sub>X package also be installed. This therefore adds&nbsp;complexity and is not likely to be an appropriate package for the everday user.&nbsp;Furthermore, the installation of the MusiXT<sub>E</sub>X package seems complicated. As&nbsp;stated in [14], </span><span class="font4">“If you are not familiar with T<sub>E</sub>X at all, I would recommend to&nbsp;find another software package to do musical typesetting” </span><span class="font33">It is however fair to say&nbsp;that once MusiXT<sub>E</sub>X is mastered, it is an excellent package to typeset music.</span></p><h2><a name="bookmark9"></a><span class="font8">2.3 Commercial OMR Systems</span></h2>
<p><span class="font33">This section provides a description of several commercial OMR software packages. Three of those packages were tested and a description of how they performed in comparison to one another is provided. In section 2.5, we will thoroughly discuss how OMR systems can be evaluated. Therefore this section only provides a user's perspective of how well each application performed.</span></p>
<p><span class="font33">1. SmartScore (Musitek)</span></p>
<p><span class="font33">2. SharpEye (Musicwave)</span></p>
<p><span class="font33">3. Notescan in Nightingale (Mac only)</span></p>
<p><span class="font33">4. SightReader in Finale</span></p>
<p><span class="font33">5. Photoscore in Sibelius (Neuratron)</span></p>
<p><span class="font33">Midiscan by Musitek was perhaps one of the first commercial OMR applications and first appeared in 1991, running on the Windows 3.1 platform. In 1998 it&nbsp;was renamed to SmartScore and in 1999 a version for the Macintosh operating&nbsp;system was released.</span></p>
<p><span class="font33">Three of the aforementioned OMR systems have evaluation versions and those were tested. All OMR systems were tested with the first page from</span></p>
<p><span class="font33">Beethoven's “Grande Sonate Pathetique” scanned at a resolution of 300dpi and saved in a jpg format. All three systems failed to open the j pg file so the image&nbsp;had to be converted to a greyscale bitmap.</span></p>
<p><span class="font33" style="font-weight:bold;">SmartScore from Musitek: </span><span class="font33">The score was opened and the image was processed by Smartscore and within 15 seconds, two images were displayed: one with the original score and one with the recognised score. Additionally, SmartScore&nbsp;is capable of playing the recognised score in a MIDI format. However, since there&nbsp;were several errors present in the processed score, the playback of the original&nbsp;score did not at all sound like it should have. Since it failed to properly identify&nbsp;a rest in the second measure, the left and right hand weren't synchronised and&nbsp;so the MIDI file sounded incorrect. (<a href="http://www.sibelius.com">www.sibelius.com</a>)</span></p>
<p><span class="font33" style="font-weight:bold;">SharpEye: </span><span class="font33">A lack of attention given to the graphical user interface of this application made it harder to visualise the recognised partition. However, the&nbsp;recognition of Beethoven's Sonate was accurate and there were few mistakes&nbsp;present in the recognised version. On this particular tested score, SharpEye&nbsp;performed best in contrast to the other tested applications. Although its GUI</span></p>
<p><span class="font33">was poorly designed, it does off an attractive feature which enables the user to modify the recognised score. This can be useful for several reasons including:&nbsp;notes needing correction after the recognition process or an artist purposely&nbsp;wanting to make modifications to the recognised score. (<a href="http://www.visiv.co.uk">www.visiv.co.uk</a>)</span></p>
<p><span class="font33" style="font-weight:bold;">PhotoScore in Sibelius: </span><span class="font33">The application did not perform well at recognising Beethoven's Sonate. In particular, it missed many accidentals, chords and other features present in the original score. This was by far the worst OMR&nbsp;system tested. This application has a nice feature which highlights all of the&nbsp;stave lines when the image is initially opened. (<a href="http://www.sibelius.com">www.sibelius.com</a>)</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-1.jpg" style="width:274pt;height:308pt;"/>
<p><span class="font33">Figure 2.3: SmartScore Screenshot</span></p><h2><a name="bookmark10"></a><span class="font8">2.4 Music Notation File Formats</span></h2>
<p><span class="font33">The Musical Instrument Digital Interface (MIDI) standard was first proposed by Dave Smith in 1981 and defines a way for music to be represented in a digital&nbsp;format and can be played by computers. Musical notes can be represented by&nbsp;different electronic musical instruments such as a synthesiser, guitar and many&nbsp;other instruments. This standard has been widely used and is still in use today&nbsp;but it suffers certain drawbacks. The MIDI format does not represent stem</span></p>
<p><span class="font33">directions, beams, repeats and other aspects in music notation [16]. Therefore the MIDI format was not appropriate for systems in which such information&nbsp;was to be preserved.</span></p>
<p><span class="font33">As a consequence, work was carried out to create a standardised music format and the Notation Interchange File Format (NIFF) pro ject was launched in 1994. This therefore meant that different music packages (whether typesetting,&nbsp;OMR or editing) would be able to share a standard format, adding a layer of&nbsp;compatibility between various music applications. [17]</span></p>
<p><span class="font33">The NIFF standardised music format was the only one to succeed but its use was limited and its format was not being maintained. Consequently, a new&nbsp;music format called MusicXML was developed and in 2004, its version 1.0 was&nbsp;released. This format is more flexible in terms of representing music and as a&nbsp;result this format has been adopted by music software applications (presently&nbsp;supported by at least 55 applications) including: Finale 2006, SharpEye Music&nbsp;Reader, LilyPond and many others. MusicXML is not open source but it is&nbsp;available under a royalty-free license from Recordare. [16]</span></p>
<p><span class="font33">The MIDI format is a useful format if one only wants to play a representation of a sound file on a computer but if the aim is to typeset or digitally archive&nbsp;a music score, then a format such as MusicXML should be considered. As the&nbsp;requirements for this pro ject are to play a scanned music score in a MIDI format,&nbsp;the MIDI format will be adopted. The MusicXML format will also be supported&nbsp;due to its robustness in representing musical symbols and its compatibility with&nbsp;other software packages.</span></p><h2><a name="bookmark11"></a><span class="font8">2.5 Evaluation of OMR Systems</span></h2>
<p><span class="font33">As several OMR software packages have been implemented, it is important to set standards as to how such systems should be evaluated. Optical character&nbsp;recognition systems can be evaluated according to the total percentage of correctly identified symbols, but this is not a valid evaluation technique for OMR&nbsp;systems. For example, if an accidental is incorrectly identified and the output&nbsp;produced is a MIDI file, it will not sound correctly, when in fact there is only a&nbsp;small mistake [8]. Therefore other evaluation techniques must be used for OMR&nbsp;systems in order to benchmark OMR systems. If each author evaluates their&nbsp;system with different sets of music scores, then it is hard to tell how systems&nbsp;perform in contrast to each other. Furthermore, authors will usually use music&nbsp;scores on which their system performed well in order to show off the performance&nbsp;of their system.[5]</span></p>
<p><span class="font33">In order to overcome the lack of ways to evaluate OMR systems, the Interactive Music Network proposed a framework to evaluate such systems. They introduced a “Quick-Test” that would enable the judgement of the capability of&nbsp;an OMR system. The OMR system being benchmarked is given a music score&nbsp;made-up purely on the basis for testing the capability and performance of OMR&nbsp;systems. Version 0.1 of the OMR “Quick-Test” consists of three pages with&nbsp;musical features such as: time signatures, notes, beams, keys signatures, clefs,&nbsp;etc.</span></p>
<p><span class="font33">A more thorough methodology to evaluate OMR systems is described in [5]. This approach involves using a set of seven images (The first page is shown in&nbsp;figure 2.4) chosen from a music database that have the following features:</span></p>
<p><span class="font33">1. Monophonic music</span></p>
<p><span class="font33">2. Font variability</span></p>
<p><span class="font33">3. Music symbols frequently used in classical music</span></p>
<p><span class="font33">4. Variable density of musical symbols</span></p>
<p><span class="font33">5. Irregular groups of notes (triplets ...)</span></p>
<p><span class="font33">6. Small notes with or without accidentals (grace notes)</span></p>
<p><span class="font33">7. Different types of barlines</span></p>
<p><span class="font33">8. Clef and time signature change</span></p>
<p><span class="font33">9. Slurs; single and nested</span></p>
<p><span class="font33">The OMR system is evaluated by using different categories in which each have a corresponding weight assigned. The weights were determined from the&nbsp;results of a survey that was completed at a conference by a group of experts&nbsp;and users of OMR systems. The categories and the weights are shown in table&nbsp;2.1</span></p>
<table border="1">
<tr><td>
<p><span class="font33">Note with pitch and duration (10)</span></p></td><td>
<p><span class="font33">Rests (10)</span></p></td></tr>
<tr><td>
<p><span class="font33">Notes with accidentals (7)</span></p></td><td>
<p><span class="font33">Groups of beamed notes (10)</span></p></td></tr>
<tr><td>
<p><span class="font33">Time signature and time change (10)</span></p></td><td>
<p><span class="font33">Key signature and key signature change (10)</span></p></td></tr>
<tr><td>
<p><span class="font33">Symbols below or above notes (5)</span></p></td><td>
<p><span class="font33">Grace notes (5)</span></p></td></tr>
<tr><td>
<p><span class="font33">Slurs and bends (7)</span></p></td><td>
<p><span class="font33">Augmentation dots (10)</span></p></td></tr>
<tr><td>
<p><span class="font33">Clefs (10)</span></p></td><td>
<p><span class="font33">Irregular notes groups (10)</span></p></td></tr>
<tr><td>
<p><span class="font33">Number of measures (10)</span></p></td><td>
<p><span class="font33">Number of staves (10)</span></p></td></tr>
</table>
<p><span class="font33">Table 2.1: Categories and associated weights considered in OMR evaluation</span></p>
<p><span class="font33">The evaluation and result analysis of the OMR system is then conducted by using a set of metrics designed for this purpose. The following evaluation&nbsp;metrics are used for each category described above.</span></p>
<p><span class="font33">1. The total number of </span><span class="font4">expected </span><span class="font33">complete symbols or relationships in the&nbsp;original score</span></p>
<p><span class="font33">2. The total number of </span><span class="font4">correct (</span><span class="font33">N </span><span class="font4">) </span><span class="font33">symbols or relationships identified in the&nbsp;reconstructed score in comparison to the original score</span></p>
<p><span class="font33">3. The total number of </span><span class="font4">added (</span><span class="font33">n</span><span class="font29">1 </span><span class="font4">) </span><span class="font33">symbols or relationships in the reconstructed score in comparison to the original score</span></p>
<p><span class="font33">4. The total number of </span><span class="font4">wrongly identified (</span><span class="font33">n</span><span class="font2">f </span><span class="font4">) </span><span class="font33">symbols or relationships in&nbsp;the reconstructed score in comparison to the original score</span></p>
<p><span class="font33">5. The total number of </span><span class="font4">missed (</span><span class="font33">n</span><span class="font2"><sub>m</sub> </span><span class="font4">) </span><span class="font33">symbols or relationships in the reconstructed score in caparison to the original score.</span></p>
<p><span class="font33">Then for each category, the following equation can be expressed:</span></p>
<p><span class="font33">N = n </span><span class="font29">1 </span><span class="font33">+ n </span><span class="font2">f </span><span class="font33">+ n </span><span class="font2">m</span></p>
<p><span class="font33">A table is then created based upon the results from each category and metric. This therefore enables us to compare and contrast how well different OMR&nbsp;systems did in relation to one another, and in which categories they performed&nbsp;best. [8] shows results conducted on SmartScore, SharpEye 2 and the O</span><span class="font29"><sup>3</sup> </span><span class="font33">MR&nbsp;system (described in section 2.8.6) with this testing framework.</span></p><h2><a name="bookmark12"></a><span class="font8">2.6 A Typical OMR Process</span></h2>
<p><span class="font33">Although there is no set standard as to how an OMR system should be implemented, most authors choose to adopt the following steps.</span></p>
<p><span class="font33">1. Digitise music score by means of a scanner</span></p>
<p><span class="font33">2. Apply filters to the image</span></p>
<p><span class="font33">3. Identification and removal of stave lines</span></p>
<p><span class="font33">4. Identification and segmentation of elementary symbols</span></p>
<p><span class="font33">5. Reconstruction and classification of music symbols</span></p>
<p><span class="font33">6. Generation of symbolic representation into a symbolic format for music&nbsp;notation (MIDI, MusicXML, etc. )</span></p>
<p><span class="font33">The first step is to acquire the music score in a digital format by means of a scanner. Once the image is stored on the computer in a digital format (such as&nbsp;a bmp, jpg, tiff ) filters are applied to the image in order to improve the quality,&nbsp;ensuring that the best results are obtained when that image is further processed&nbsp;for recognition. An example of such a filter may involve converting the image&nbsp;to a binary format. The next step is to identify the staves within the image.&nbsp;While some researchers claim this step to be unnecessary, most authors disagree&nbsp;and view this step as being a vital one. Depending on which method is used to&nbsp;further process the image, the staves may be removed at this point. The image&nbsp;is then segmented and elementary symbols are then recognised and classified.&nbsp;A representation of the music score is then created (MIDI or MusicXML) by&nbsp;using the information from the classification module.</span></p><h2><a name="bookmark13"></a><span class="font8">2.7 Issues/Problems Associated with OMR Systems</span></h2><h3><a name="bookmark14"></a><span class="font34">2.7.1 Graphic Quality and Print Faults</span></h3>
<p><span class="font33">There are many factors that can influence the quality of printed music. For instance, old printed music may have stave lines which are fading away, hence,&nbsp;affecting the quality of the printed music and in turn making it harder for an&nbsp;OMR system to perform its stave line recognition process. Skewed stave lines&nbsp;can make it difficult for an OMR system to perform the stave line recognition.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-2.jpg" style="width:345pt;height:491pt;"/>
<p><span class="font33">Figure 2.4: The first page of the score used to evaluate OMR systems</span></p>
<p><span class="font33">Therefore stave line detection algorithms must take into account skewed stave lines. Several ways have been developed to detect skewed stave lines of which&nbsp;one uses a Fast Fourier Transform [3]. In some older printed music, the thickness&nbsp;of the stave lines varied and this is therefore something that needs to be taken&nbsp;into account. It is possible for a note to be covering a space and a line, therefore&nbsp;making it hard for the OMR system to decide if the note is on a line or on a&nbsp;space.</span></p><h3><a name="bookmark15"></a><span class="font34">2.7.2 Handwritten Partitions</span></h3>
<p><span class="font33">Handwritten partitions are harder to process for various reasons. Each author has their own way of writing music partitions and this is analogous to everyone&nbsp;having their own signature. As handwritten partitions are not printed by computer systems, there can be variations in the size of stave lines and music notes,&nbsp;adding to the complexity of the OMR system. Due to the complexity involved&nbsp;with the recognition of handwritten partitions, they will not be taken account&nbsp;in this pro ject.</span></p><h2><a name="bookmark16"></a><span class="font8">2.8 OMR Research/Projects</span></h2><h3><a name="bookmark17"></a><span class="font34">2.8.1 Carter (1988)</span></h3>
<p><span class="font33">Carter's most important contribution was the idea of segmenting the music score image by using a Line Adjacency Graph (LAG). This method involves scanning&nbsp;the image horizontally and vertically, searching for paths of black pixels. Graph&nbsp;nodes correspond to the unions of adjacent and vertically overlapping segments&nbsp;while arcs define the overlapping of different segments in an adjacent column&nbsp;(junctions) [7]. The graph obtained is then analysed to detect the stave line&nbsp;and symbols that overlap the stave line. By using this technique, the following&nbsp;features are obtained:</span></p>
<p><span class="font33">1. Identification of empty areas within the staff.</span></p>
<p><span class="font33">2. Identification of the staff (even if the image is skewed up to 10 degrees).</span></p>
<p><span class="font33">3. Identification of the stave lines that are slightly bent, broken or in which&nbsp;the thickness varies.</span></p>
<p><span class="font33">The phases of his OMR system can be described as follows:</span></p>
<p><span class="font33">1. A LAG is produced by isolating the empty parts of the stave lines. This&nbsp;stage also isolates all the symbols and groups of connected or overlapping&nbsp;symbols.</span></p>
<p><span class="font33">2. The objects are then classified according to the bounding box size and the&nbsp;organisation of their constituent sections.</span></p><h3><a name="bookmark18"></a><span class="font34">2.8.2 Fujinaga (1988)</span></h3>
<p><span class="font33">Fujinaga proposed an OMR system which heavily relied on the projections of images to identify and classify the staves and symbols. As opposed to most&nbsp;other systems, his system does not require the removal of stave lines. The staves</span></p>
<p><span class="font33">are identified by using the Y-projection of the music score and the symbols are identified by using a combination of projections. Fujinaga also provides a formal&nbsp;way of describing music notation by means of a context-free grammar. [8]</span></p>
<p><span class="font33">The phases of his OMR system can be described as follows:</span></p>
<p><span class="font33">1. The stave line is located by taking the Y-projection of the image (described&nbsp;in section 2.12) in which groups of five peaks are present.</span></p>
<p><span class="font33">2. Symbols are then detected by taking the X-projection of the image (also&nbsp;described in section 2.12). Values that are greater than the background&nbsp;noise suggest the presence of music symbols. The syntax proposed is then&nbsp;applied to support the detection of symbols.</span></p>
<p><span class="font33">3. The X- and Y-projections are then applied to classify the symbols. Features such as the width, height, number of peaks and number of pixels in&nbsp;the X-Pro jection are all considered as part of the classification process. He&nbsp;also suggests considering the first and second derivatives of the projection&nbsp;profiles.</span></p><h3><a name="bookmark19"></a><span class="font34">2.8.3 Roth (1994)</span></h3>
<p><span class="font33">In 1994, Roth developed an OMR system whose output was to be used with the Lipsia music notation editor (developed at ETTH Zurich) in order to digitally&nbsp;reproduce the original music score. The OMR architecture is built upon the&nbsp;seven following steps:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;Rotation - If the scanned image is skewed, it is rotated. This step is not&nbsp;automated in the application and a separate program is used to rotate the&nbsp;image.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;Vertical run length statistics - The distance between two staffs and the&nbsp;thickness of the stave lines are calculated by using elementary statistics&nbsp;that are described in section 2.11. From the those two parameters, the&nbsp;height of the staves can be determined.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;Locate and delete stave lines - By taking the Y-projection of the image,&nbsp;the stave lines are located by searching for groups of five peaks. The stave&nbsp;lines are then removed by erasing the lines of width calculated in step 2.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;Locate and delete vertical lines - The removal of the vertical lines (stems,&nbsp;bar lines ...) is accomplished by taking the X-projection (described in&nbsp;section 2.12) of the image and also by using Mathematical Morphology.</span></p>
<p><span class="font33">5. &nbsp;&nbsp;&nbsp;Connected component labelling - All remaining components are then identified and labelled according to where they were found in the image.</span></p>
<p><span class="font33">6. &nbsp;&nbsp;&nbsp;Symbol recognition -The components from step 5 are then classified and&nbsp;recognised by using a set of graphic rules.</span></p>
<p><span class="font33">7. &nbsp;&nbsp;&nbsp;Lipsia document generation - The recognised symbols in step 6 are then&nbsp;written in the Lipsia file format.</span></p>
<p><span class="font33">As mentioned in [9], the results were satisfactory although no percentage or benchmarks were given. All the rules are hard-coded and this therefore makes&nbsp;it hard to add new symbols to the system, Roth does suggest designing a formal&nbsp;language describing rules for each symbol as an extension to his system.</span></p><h3><a name="bookmark20"></a><span class="font34">2.8.4 &nbsp;&nbsp;&nbsp;Bainbridge (1991—1997)</span></h3>
<p><span class="font33">In [1] and [2], Bainbridge describes a system capable of recognising different</span></p>
<p><span class="font33">music notations such as the Common Music Notation (CMN), percussion and</span></p>
<p><span class="font33">tabulature. He first describes a way to detect the stave lines by means of taking</span></p>
<p><span class="font33">a horizontal projection of the black pixels on each row of the image. Before</span></p>
<p><span class="font33">proceeding to removal of the stave lines, an OCR processing phase is conducted</span></p>
<p><span class="font33">in order to remove all text present in the score. Bainbridge also introduces a</span></p>
<p><span class="font33">language called Primela that has the following properties:</span></p>
<p><span class="font33">1. Graphical description</span></p>
<p><span class="font33">2. Variable instantiation</span></p>
<p><span class="font33">3. Matching control</span></p>
<p><span class="font33">This language allows the specification of how to assemble primitives together.</span></p>
<p><span class="font33">It is common to find systems in which primitives have been hard-coded, thus</span></p>
<p><span class="font33">making it difficult to extend the system. The Primela language supports four</span></p>
<p><span class="font33">different types of pattern recognition techniques which are categorised as follows:</span></p>
<p><span class="font33">Pro jections, Hough transform, template matching and slicing techniques.</span></p>
<p><span class="font33">He states that the system was tested with different examples of CMN and</span></p>
<p><span class="font33">Georgian chant music and that the results obtained were encouraging. The</span></p>
<p><span class="font33">solution of breaking down the problem into fragments is novel and that it is</span></p>
<p><span class="font33">more flexible than the earlier ad-hoc methods. However, the computation cost</span></p>
<p><span class="font33">associated with those techniques increases [7].</span></p><h3><a name="bookmark21"></a><span class="font34">2.8.5 &nbsp;&nbsp;&nbsp;Ng and Boyle (1992-2002)</span></h3>
<p><span class="font33">The Automatic Music Score Recogniser (AMSR) was developed in 1994 at the</span></p>
<p><span class="font33">University of Leeds. The approach used in this system is based on reverse</span></p>
<p><span class="font33">engineering. Usually, a composer will first write the beam, then the stem and</span></p>
<p><span class="font33">then add features such as slurs. The AMSR first looks for thick horizontal</span></p>
<p><span class="font33">features such as slurs and then find stems and finally beams. Hence complex</span></p>
<p><span class="font33">music symbols are decomposed into primitive ones easing the classification of</span></p>
<p><span class="font33">the symbols [7].</span></p>
<p><span class="font33">The system can be described as follows:</span></p>
<p><span class="font33">1. The image is preprocessed and converted from a greyscale-scale image to a&nbsp;binary image by means of a thresholding function. If the image is skewed,&nbsp;the rotation of the image is corrected. As this system relies on projection&nbsp;methods, this step is crucial. The stave line parameters are calculated (as&nbsp;described in the above OMR systems).</span></p>
<p><span class="font33">2. In this sub-segmentation module, composite symbols are divided into&nbsp;primitive elements such as note heads, beams, stems. If those symbols cannot be properly classified, they are then passed into the sub-segmentation&nbsp;module again to be further broken down.</span></p>
<p><span class="font33">3. A k-Nearest Neighbour classifier is used to detect primitives. Sub-segmentation&nbsp;techniques are used to reconstruct the primitive symbols.</span></p>
<p><span class="font33">4. The output is written in a format called ExpMidi. This format is compatible with MIDI, however it has the benefits of being more expressive than&nbsp;the standard Midi format and is able to describe features such as accents.</span></p><h3><a name="bookmark22"></a><span class="font34">2.8.6 The </span><span class="font9">O<sup><a name="footnote1"></a><a href="#bookmark84">1</a> <a name="footnote2"></a><a href="#bookmark85">2</a> <a name="footnote3"></a><a href="#bookmark86">3</a></span><span class="font34"></sup>MR</span></h3>
<p><span class="font33">As opposed to the aforementioned OMR systems, the Object Oriented Optical Music Recognition (O</span><span class="font29"><sup><a name="footnote3"></a><a href="#bookmark86">3</a></sup> </span><span class="font33">MR) does not remove staves. The approach of the O</span><span class="font29"><sup><a name="footnote3"></a><a href="#bookmark86">3</a></span><span class="font33"></sup>MR system is based on the pro jection profiles to extract basic symbols that&nbsp;contribute to the formation of a music partition. This architecture is only concerned with the music written in the western style notation and in which only&nbsp;monophonic music with five stave lines are present. There are four steps involved&nbsp;in this OMR architecture and are as follows:</span></p>
<p><span class="font33">1. Segmentation</span></p>
<p><span class="font33">2. Basic Symbol Recognition</span></p>
<p><span class="font33">3. Music Notation Symbol Recognition</span></p>
<p><span class="font33">4. Music Notation Model Refinement</span></p>
<p><span class="font33">The segmentation process of the O</span><span class="font29"><sup><a name="footnote3"></a><a href="#bookmark86">3</a></sup> </span><span class="font33">MR architecture can be broken down into</span></p>
<p><span class="font33">three further levels. Level-0 involves a tuning process in which two parameters (thickness of stave lines and distance between stave lines) are obtained. By using&nbsp;those parameters, the music score is then decomposed into a series of different&nbsp;images in which only one stave is present. Level-1 then works on the image&nbsp;segments produced by Level-0 and extracts vertical image segments containing&nbsp;music symbols. The next level then decomposes the input from Level-1 into&nbsp;basic symbols.</span></p>
<p><span class="font33">The basic symbol recognition module is responsible for recognising the basic symbols that have been output from Level-2 of the segmentation process. The&nbsp;output from Level-2 of the segmentation process is then normalised and input&nbsp;into a neural network.</span></p>
<p><span class="font33">The music notation symbol recognition step maps the basic symbols into elementary components of music notation symbols. For example, a stem might&nbsp;be mapped to to a beam.</span></p>
<p><span class="font33">The last part of the system consists of constructing a music model by refining the process used in the previous step. [4]</span></p><h2><a name="bookmark23"></a><span class="font8">2.9 Optical Music Recognition using Kd-tree Decomposition</span></h2>
<p><span class="font33">This is a project that was undertaken by Kerry Peake at the University of Birmingham and focuses on using a Kd-tree decomposition method and Euclidean Distance Metric to retrieve and identify symbols in a music partition. The&nbsp;architecture of the system can be broken down into the following 3 stages [20]:</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;Symbol Analysis - Most of the symbol analysis is performed by using a&nbsp;Kd-tree decomposition method along with Euclidean distance metrics.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;Contextual Post-processing - Once all the symbols are identified, they can&nbsp;be interpreted musically from which a Midi file and a MusiXT<sub>E</sub>Xfile are&nbsp;generated.</span></p><h2><a name="bookmark24"></a><span class="font8">2.10 Music Sheet Reader</span></h2>
<p><span class="font33">This “Music Sheet Reader” is the implementation of an OMR system by Yong Li at the University of Birmingham. The implementation of the project uses a&nbsp;segmentation based approach along with a neural network to classify so-called&nbsp;“immutable” symbols. The implementation was written in Java and the basic&nbsp;architecture of the system is as follows [21]:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;Preprocessing - If the scanned image is in colour, the image is binarised by&nbsp;using a thresholding mechanism in order to convert the image to a binary&nbsp;one (black and white).</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;De-skewing and stave detection.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;Music symbol identification - This step is decomposed into two different&nbsp;categories: immutable and mutable symbols. Immutable symbols such&nbsp;as note heads and rests are ones for which their dimension is consistent&nbsp;throughout the music score and mutable symbols are ones such as slurs,&nbsp;beams and ties for which their dimension is inconsistent throughout the&nbsp;score. A three-layer MLP network is used in this step to classify immutable&nbsp;symbols. The mutable symbols are detected at the interpretation stage.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;Output - The output generated by this program is in both a midi and&nbsp;MusiXTeX format. A package called jMusic was used to generate the&nbsp;midi file.</span></p><h2><a name="bookmark25"></a><span class="font8">2.11 Run Length Encoding</span></h2>
<p><span class="font33">The Run Length Encoding Algorithm is briefly described in this section as it will be used to determine the thickness of the stave lines in the score and the distance&nbsp;between two stave lines. We will see in chapter 2 how the RLE algorithm can&nbsp;be applied to a music score in order to detect the parameters mentioned above.</span></p>
<p><span class="font33">The RLE algorithm is perhaps one of the simplest encoding algorithms which can be applied to a stream of data and works by counting each consecutive&nbsp;occurrences of a same value in a sequence of values. We will later refer to this&nbsp;as being a ‘run'.</span></p>
<p><span class="font33">Consider the following binary values which could perhaps represent a sequence of pixels in a black and white image (where a black pixel has the value 0 and a white pixel has the value 1):</span></p>
<p><span class="font33">1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0</span></p>
<p><span class="font33">By applying the RLE algorithm, this sequence of numbers could therefore be represented as a set of tuples of the form: value, occurrence</span></p>
<p><span class="font33">(1,3),(0,6),(1,8),(0,3)</span></p><h2><a name="bookmark26"></a><span class="font8">2.12 X- and Y- Pro jection Definition</span></h2>
<p><span class="font33">It is important to understand the underlying concepts of the x and y-pro jections of an image as they are heavily used by several modules in the OMR design.</span></p>
<p><span class="font33">The X-Projection is the pro jection of an image onto the the X-axis. The result is a vector whose i</span><span class="font1"><sup>th</sup> </span><span class="font33">component is the sum of all black pixels in the i</span><span class="font1"><sup>th</sup> </span><span class="font33">column&nbsp;of the image.</span></p>
<p><span class="font33">The Y-Pro jection is the projection of an image onto the Y-axis. The result is a vector whose i</span><span class="font1"><sup>th</sup> </span><span class="font33">component is the sum of all black pixels in the i</span><span class="font1"><sup>th</sup> </span><span class="font33">row of&nbsp;the image [6].</span></p>
<p><span class="font33">Let us now look at an example of calculating the X and Y-Projections of the image in figure 2.5. The image in this figure is 6 pixels wide by 7 pixels high&nbsp;and represents the letter ‘A'. Calculating the X-Pro jection of this image will&nbsp;yield an array consisting of 6 elements with the following values:</span></p>
<p><span class="font33">0, 6, 3, 3, 6, 0</span></p>
<p><span class="font33">Calculating the Y-Pro jection of this image yields an array consisting of 7 elements with the following values:</span></p>
<p><span class="font33">2, 4, 2, 4, 2, 2, 2</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-3.png" style="width:121pt;height:141pt;"/>
<p><span class="font33">Figure 2.5: Example of a 6 pixel wide by 7 pixel high image</span></p><h2><a name="bookmark27"></a><span class="font8">2.13 Discrete Fourier Transform (DFT)</span></h2>
<p><span class="font33">An interesting approach to detect the angle by which staves are skewed in an image is discussed in [3]. This method is of particular interest to us as we want&nbsp;to be able to determine by how much a music score needs to be rotated in order&nbsp;to realign the stave(s) in such a way that they are parallel to the horizontal axis&nbsp;of the image. The method described in [3] uses an approach based on the 2D</span></p>
<p><span class="font33">Discrete Fourier transform of an image and we will now discuss the basics of the 2D FT in this section.</span></p><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-4.jpg" style="width:388pt;height:334pt;"/>
<p><span class="font33">Figure 2.6: Top Left: 10 horizontal black and white lines. Top Right: 10</span></p>
<p><span class="font33">black and white lines rotated by 6</span><span class="font2">^ </span><span class="font33">clockwise. Bottom Left and Right: Fourier Transform of the images directly above them</span></p></div><br clear="all"/>
<p><span class="font33">The Fourier Transform for an N </span><span class="font4">x </span><span class="font33">N image and its inverse are defined as follows:</span></p>
<p><span class="font2">N-</span><span class="font29">1 </span><span class="font2">N-</span><span class="font29">1</span></p>
<p><span class="font33">F (u,v) = &nbsp;&nbsp;&nbsp;f (x,y) e</span><span class="font29"><sup>(</span><span class="font2">-</span><span class="font29">2</span><span class="font1">nj</span><span class="font29">(</span><span class="font1">ux</span><span class="font29"></sup>+</span><span class="font1"><sup>vy</span><span class="font29">)</span><span class="font1">/N</span><span class="font29"></sup>)</span></p>
<p><span class="font2">x </span><span class="font29">=0 </span><span class="font2">y </span><span class="font29">=0</span></p>
<p><span class="font33">and</span></p>
<p><span class="font2">N-</span><span class="font29">1</span><span class="font2">N-</span><span class="font29">1</span></p>
<p><span class="font33">f (x,y) = &nbsp;&nbsp;&nbsp;F (u,v) e</span><span class="font29"><sup>(</sup>+<sup>2</span><span class="font1">nj</span><span class="font29">(</span><span class="font1">ux</span><span class="font29"></sup>+</span><span class="font1"><sup>vy</span><span class="font29">)</span><span class="font1">/N</span><span class="font29"></sup>)</span></p>
<p><span class="font2">x </span><span class="font29">=0 </span><span class="font2">y </span><span class="font29">=0</span></p>
<p><span class="font33">where j = &nbsp;&nbsp;&nbsp;</span><span class="font4">— </span><span class="font33">1</span></p>
<p><span class="font33">F(u, v) is a complex number and is represented by a magnitude and phase rather than a real and imaginary component</span></p>
<p><span class="font33">and</span></p>
<p><span class="font33">f (x, y ) is a function representing the pixels of the image at coordinates (x, y ) and has a real value [19].</span></p>
<p><span class="font33">The magnitude and phases of the FT are calculated as follows:</span></p>
<p><span class="font33" style="font-style:italic;">Magnitude (F</span><span class="font33">) = &nbsp;&nbsp;&nbsp;real (F)</span><span class="font29"><sup>2</sup> </span><span class="font33">+ </span><span class="font33" style="font-style:italic;">imaginary (F</span><span class="font33">)</span><span class="font29"><sup>2</sup></span></p>
<p><span class="font33" style="font-style:italic;">Phase (F) = tan</span><span class="font2" style="font-style:italic;">-</span><span class="font30" style="font-style:italic;">1 </span><span class="font4" style="font-style:italic;">(</span><span class="font4" style="font-style:italic;text-decoration:line-through;"><sup>(F</span><span class="font4" style="text-decoration:line-through;"></sup>&gt;</span><span class="font32" style="font-weight:bold;">)</span></p>
<p><span class="font33" style="font-style:italic;">real (F)</span></p>
<p><span class="font33" style="font-style:italic;">The magnitude indicates how much of a certain frequency component is present whilst the phase indicates the loca t ion of the frequency components.&nbsp;From now on, we will refer to the Fourier Transform as being its magnitude as&nbsp;we will not be using the phase information of the transform.</span></p>
<p><span class="font33" style="font-style:italic;">The magnitude image of the Discrete Fourier Transform has the following properties. The u-axis represents the horizontal frequency component and runs&nbsp;from left to right through the centre of the image. The v-axis represents the&nbsp;vertical frequency component and runs from bottom to top through the centre&nbsp;of the image. The centre of the image corresponds to the origin of the frequency&nbsp;coordinate system.</span></p>
<p><span class="font33" style="font-style:italic;">The Fourier Transform attempts to represent an image as a summation of cosine-like images. The image such as the one to the top left in figure 2.6 is a&nbsp;pure cosine image and has a simple Fourier Transform. The image to the bottom left is the Fourier Transform of the image directly above it and we notice&nbsp;that it has a vertical line running through the middle of the image consisting&nbsp;of bright white spots. The image to the top right of figure 2.6 is identical to&nbsp;</span><span class="font33">its neighbour except that it is rotated by an angle of 6</span><span class="font2">° </span><span class="font33">clockwise. The Fourier&nbsp;Transform of the image is shown directly beneath it and we still notice a line&nbsp;consisting of bright white spots, but it is no longer perfectly vertical. This time,&nbsp;it is at an angle to the y-axis and appears to be rotated by the same angle as the&nbsp;original image. This is exactly what will enable to detect if staves are rotated&nbsp;in an image.</span></p>
<p><span class="font33">Some properties of the transform as mentioned in [3] are:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;The Fourier Transform is symmetric to the origin. That is: </span><span class="font33" style="font-style:italic;">F</span><span class="font33">(</span><span class="font4">-</span><span class="font33" style="font-style:italic;">x,</span><span class="font4"> -</span><span class="font33" style="font-style:italic;">y</span><span class="font33">&gt; =&nbsp;</span><span class="font33" style="font-style:italic;">F</span><span class="font33">(</span><span class="font33" style="font-style:italic;">x, y</span><span class="font33">&gt;</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;The most distinct feature in the Fourier Transform of a music score is the&nbsp;line consisting of bright white spots at an angle to the vertical axis.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;The angle between the strong component in the Fourier transform can&nbsp;be determined by locating two strong components (i.e the brightest white&nbsp;spots&gt; which are the furthest away from one another. The angle can thus&nbsp;be calculated as follows given two coordinates (</span><span class="font33" style="font-style:italic;">x</span><span class="font2"><sub>1</span><span class="font33" style="font-style:italic;"></sub>, y</span><span class="font2"><sub>1</span><span class="font33"></sub>&gt; and (</span><span class="font33" style="font-style:italic;">x</span><span class="font2"><sub>2</span><span class="font33" style="font-style:italic;"></sub>,y</span><span class="font2"><sub>2</span><span class="font33"></sub>&gt;:</span></p><div>
<p><span class="font33" style="font-style:italic;">d</span><span class="font33"> = sin</span></p></div><br clear="all"/><div>
<p><span class="font2">-</span><span class="font29">i </span><span class="font33" style="text-decoration:underline;">y</span><span class="font29" style="text-decoration:underline;">i </span><span class="font4" style="text-decoration:underline;"><sup>-</sup> </span><span class="font33" style="font-style:italic;text-decoration:underline;">yo,</span></p></div><br clear="all"/><div>
<p><span class="font33" style="font-style:italic;">x</span><span class="font29">1 </span><span class="font4">- </span><span class="font33" style="font-style:italic;">x</span><span class="font29">2</span></p></div><br clear="all"/><h2><a name="bookmark28"></a><span class="font8">2.14 Artificial Neural Networks</span></h2>
<p><span class="font33">Artificial neural networks attempt to detect patterns or trends which are too complex for other computing techniques to model. They attempt to mimic the&nbsp;behaviour of the human brain within certain limits. While it is estimated that&nbsp;the brain is made up a over a hundred billion neurons, artificial neural networks&nbsp;generally consist of a few hundred neurons. Neural networks have proven to be&nbsp;accurate for Optical Character Recognition applications. Most of the discussion&nbsp;to follow is based on [11] and [12].</span></p><h3><a name="bookmark29"></a><span class="font34">2.14.1 Supervised and Unsupervised Training</span></h3>
<p><span class="font33">As in most machine learning techniques, neural networks must be trained in order to provide useful outputs. A neural network is trained by collecting different data sets and feeding them into the neural network in a special “learning mode”&nbsp;and we will later see how the “learning mode” of the neural network affects the&nbsp;internal state of the network. Additionally, we will also discuss how long the&nbsp;training of the neural network should go on for and what parameters we can&nbsp;change to train the neural network. Generally, the training of neural networks&nbsp;falls into two categories: supervised or unsupervised training.</span></p>
<p><span class="font33">In supervised training, a set of inputs along with the expected corresponding outputs are given to the neural network and this is the most common type of&nbsp;training for neural networks.</span></p>
<p><span class="font33">In unsupervised learning, the neural network is only given a set of inputs and based on those it tries to find a correlation between them to classify them into&nbsp;different categories. The Self Organising Feature Map neural network which is&nbsp;briefly mentioned in section 2.14.6 is trained in such a way.</span></p>
<p><span class="font33">Properly training a neural network is important as they are prone to memorising the training data set and when later used with unseen data sets their performance is poor. This is a phenomenon known as overfitting and will be discussed in greater detail in section 2.11. By the same token, if a neural network&nbsp;is not given enough data to train on, it will also poorly perform.</span></p><h3><a name="bookmark30"></a><span class="font34">2.14.2 The Artificial Neuron</span></h3>
<p><span class="font33">The artificial neuron is the basic unit from which a neural network is constructed. It can be viewed as a many-to-one function that will either fire or not fire based on the summed value of its input(s). Figure 2.7 shows a diagram of&nbsp;an artificial neuron.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-5.jpg" style="width:282pt;height:78pt;"/>
<p><span class="font33">The Neuron is made up of the following:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;Inputs - The inputs to the artificial neuron can come from outside the&nbsp;neural network (input layer) or from other neurons such as in multi-layered&nbsp;perceptron networks. The inputs are real-valued, i.e they take the form&nbsp;of a decimal number. Furthermore, each input is sub ject to a weight that&nbsp;is varied whilst the neural network undergoes its training phase. This&nbsp;therefore means that different inputs may have a greater impact than&nbsp;others.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;A threshold value - In the case where a non-linear artificial neuron is used,&nbsp;a threshold value is input into the neuron. It has a weight like all other&nbsp;inputs, except that the input comes from a node that always outputs a 1.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;Combination function - This is a simple linear function that calculates the&nbsp;strength of the incoming signals into the artificial neuron. It is defined as&nbsp;follows, where c is the output of the combination function with n inputs:</span></p>
<p><span class="font2">n</span></p>
<p><span class="font33">c</span><span class="font2">j </span><span class="font33">= &nbsp;&nbsp;&nbsp;w</span><span class="font2">ji </span><span class="font33">x</span><span class="font2">i</span></p>
<p><span class="font2">i</span><span class="font29">=0</span></p>
<p><span class="font33">Where x</span><span class="font2"><sub>0</sub> </span><span class="font33">. . . x</span><span class="font2"><sub>i</sub> </span><span class="font33">are the real values of the input signals and w</span><span class="font2"><sub>j0</sub> </span><span class="font33">. . . x</span><span class="font2"><sub>ji</sub> </span><span class="font33">are the respective weights for the inputs signals.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;An activation function - The activation function determines the output&nbsp;of the artificial neuron (i.e the value when it fires). There exist several&nbsp;different activation functions of which include the step, sigmoid and hyperbolic tangent functions. The output y of the artificial neuron can then&nbsp;be defined as follows:</span></p>
<p><span class="font2">ji i</span></p>
<p><span class="font33">Where &lt; is the activation function.</span></p><h3><a name="bookmark31"></a><span class="font34">2.14.3 Feedforward Neural Networks</span></h3>
<p><span class="font33">There are several types of neural networks but we shall limit our discussion to feedforward neural networks and in particular, the perceptron and multi-layer&nbsp;perceptron networks. Feedforward neural networks are arguably the most simple&nbsp;of neural networks in which information is propagated in only one direction.&nbsp;That is, information moves from the input nodes until reaching the output&nbsp;nodes. Hence in feedforward neural networks, there are no cycles or loops.</span></p>
<p><span class="font33" style="font-weight:bold;">Perceptrons</span></p>
<p><span class="font33">The perceptron was invented in 1957 at the by Frank Rosenblatt at the Cornell Aeronautical University. It is often seen as the simplest kind of feedforward&nbsp;neural network and only consists of an input and output layer. This model&nbsp;attracted lots of attention when initially introduced until it was proven that&nbsp;the perceptron was only capable of solving linearly separable problems. The&nbsp;classic “XOR Problem” illustrated by Minsky and Papert's book was used to</span></p>
<p><span class="font33">demonstrate that the perceptron was incapable of learning the XOR boolean function and led to a period of inactivity in the research area of neural networks.&nbsp;Figure 2.8 shows how the AND and OR boolean functions are linearly separable&nbsp;and the the XOR boolean function is not linearly separable.</span></p><div>
<p><span class="font12">AND Values OR Values</span></p></div><br clear="all"/><div>
<p><span class="font12">XOR Values</span></p>
<p><span class="font36">1 i &nbsp;&nbsp;&nbsp;/ x</span></p>
<p><span class="font33">/</span></p></div><br clear="all"/><div>
<p><span class="font6" style="font-weight:bold;">-X</span></p></div><br clear="all"/><div>
<p><span class="font5" style="font-weight:bold;">-X</span></p></div><br clear="all"/><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-6.jpg" style="width:96pt;height:76pt;"/></div><br clear="all"/>
<p><span class="font33">Figure 2.8: Boolean Chart: Red ”X” denotes true and Blue ”X” denotes false</span></p>
<p><span class="font33" style="font-weight:bold;">Multi-Layer Perceptrons (MLP)</span></p>
<p><span class="font33">In contrast to perceptrons, multi-layer perceptrons have one or several hidden layers. The input layer propagates its outputs to the first hidden layer which&nbsp;in turn propagates its output on to the next node until it finally reaches the&nbsp;output node. Some neural networks use more than one hidden layer, although&nbsp;one hidden layer is sufficient for most problems. Figure 2.9 shows a simple MLP&nbsp;network which can be used to solve the XOR problem.</span></p><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-7.jpg" style="width:327pt;height:76pt;"/>
<p><span class="font33">Figure 2.9: A Multi-Layer Perceptron Network</span></p></div><br clear="all"/>
<p><span class="font33">The frequently used training algorithm (backpropagation) for training the MLP network is derived mathematically by using differential calculus and we&nbsp;therefore require a differentiable activation function. We cannot use the step&nbsp;function as our activation function as it is not continuous and hence nondifferentiable, so instead we will discuss the sigmoid function. When plotted,&nbsp;the sigmoid function has an “S” shape and therefore closely resembles the step&nbsp;function.</span></p>
<p><span class="font33">The sigmoid function is defined as follows:</span></p><div>
<p><span class="font33">p</span><span class="font4"><sup>(c</span><span class="font2"></sup>j</span><span class="font33">)</span></p></div><br clear="all"/><div>
<p><span class="font33">1</span></p>
<p><span class="font33">1 + e</span><span class="font2"><sup>-</span><span class="font1">c</span><span class="font4">j</span></sup></p></div><br clear="all"/>
<p><span class="font33">Where c</span><span class="font2">j </span><span class="font33">is the combination function and e is Euler's number.</span></p>
<p><span class="font33">Furthermore when computing the derivative of the sigmoid function, we observe that its derivative is computationally easy to perform.</span></p>
<p><span class="font33">-= p </span><span class="font4"><sup>(c</span><span class="font2"></sup>j</span><span class="font4"><sup>)(1 -</sup> </span><span class="font33">p </span><span class="font4"><sup>(c</span><span class="font2"></sup>j</span><span class="font33">))</span></p>
<p><span class="font4"><sup>c</span><span class="font2"></sup>j</span></p>
<p><span class="font33" style="font-weight:bold;">Backpropagation Algorithm</span></p>
<p><span class="font33">The MLP network is trained by altering the weights in each artificial neuron in such a way that the difference between the desired output value and the output&nbsp;we get is minimised. One of the most famous algorithms to train a neural</span></p>
<p><span class="font33">network is the backpropagation algorithm and was introduced by Rumelhart, Werbos and Parker in the late 1980's. The backpropagation algorithm works as&nbsp;follows:</span></p>
<p><span class="font33">1. Initialise the network by randomly assigning weights ranging from </span><span class="font4">- </span><span class="font33">0 . 5&nbsp;to +0.5 for all neurons in each node.</span></p>
<p><span class="font33">2. Test the neural network with the input data set. After running through&nbsp;all the input data sets, an error term is calculated and weight changes&nbsp;occur at each node. The error term is usually expressed as a root mean&nbsp;squared error and can be calculated as follows:</span></p>
<p><span class="font33"><sub>1</sub> </span><span class="font2">numtrainingexamples numoutputs </span><span class="font33" style="font-style:italic;">RMSE = -</span><span class="font32" style="font-weight:bold;">&nbsp;&nbsp;&nbsp;&nbsp;E&nbsp;&nbsp;&nbsp;&nbsp;E&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font33">( target</span><span class="font2"><sub>o</sub> </span><span class="font4">- </span><span class="font33">output</span><span class="font2"><sub>o</sub> </span><span class="font33">) </span><span class="font29"><sup><a name="footnote4"></a><a href="#bookmark87">4</a></sup></span></p>
<p><span class="font4"><sup>2</sup> &nbsp;&nbsp;&nbsp;</span><span class="font2">j</span><span class="font29">=0&nbsp;&nbsp;&nbsp;&nbsp;0=1</span></p>
<p><span class="font33">3. If a certain terminating condition is met, training stops or else step 2 is&nbsp;repeated.</span></p><h3><a name="bookmark32"></a><span class="font34">2.14.4 Local Minima</span></h3>
<p><span class="font33">This is a problem that can frequently occur with the backpropagation algorithm, and many other AI techniques. It occurs when the error value for the network&nbsp;gets stuck in a local minimum not having reached its potential global minimum.</span></p>
<p><span class="font33">This is illustrated in figure 2.-0. In such a case, trying to move away from the local minimum will result in an increase of the error value. There are several&nbsp;ways of getting around this problem of which two are mentioned:</span></p>
<p><span class="font33">-. The network can be re-initialised with new random values in hope that it won't get stuck in a local minimum.</span></p><div><h3><a name="bookmark88"></a><span class="font34">RMSE</span></h3></div><br clear="all"/><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-8.jpg" style="width:154pt;height:137pt;"/>
<p><span class="font33">Figure 2.10: Local Minima</span></p></div><br clear="all"/><h3><a name="bookmark89"></a><span class="font34">Epochs</span></h3><h3><a name="bookmark33"></a><span class="font34">2.14.5 Overfitting</span></h3>
<p><span class="font33">Overfitting can occur if a neural network is left to train for a long period of time after which it memorises the training set. In such circumstances, the network&nbsp;will poorly perform when given unseen data.</span></p>
<p><span class="font33">Figure 2.11 shows a graph of the root-mean squared error versus the number of epochs that a given neural network has been trained for. The results of&nbsp;a training and validation set are presented. Initially, the error for both sets&nbsp;decreases rapidly but after the network has been trained for a certain number&nbsp;of epochs, the error for the training set carries on decreasing whilst the error for&nbsp;the validation set increases. This is an example of when overfitting occurs. We&nbsp;can therefore use this information to terminate the training algorithm. After&nbsp;each epoch, the previous state of the neural network (i.e the values of all the&nbsp;weights for each neuron) is stored and if the error rate for the validation set&nbsp;is greater than at the previous epoch, the neural network is rolled back to its&nbsp;previous state.</span></p><h3><a name="bookmark34"></a><span class="font34">2.14.6 Self Organising Feature Map (SOFM)</span></h3>
<p><span class="font33">The SOFM neural network is also known as the Kohonen neural network which is named after its inventor. The SOFM differs in several ways from MLP networks.&nbsp;The SOFM does not have any hidden layers and is trained in an unsupervised&nbsp;mode. That is, it is presented with a set of data and will then classify it&nbsp;in a set of different classes. The output from the SOFM network does not&nbsp;consist of the output of several neurons but rather one output node is selected&nbsp;as the ”winner”. As with perceptrons, SOFM's can only be applied to linearly&nbsp;separable problems. SOFM networks are typically used for OCR applications&nbsp;and hence why they are mentioned.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-9.jpg" style="width:176pt;height:152pt;"/>
<p><span class="font5">Validation Set</span></p>
<p><span class="font5">Training Set</span></p>
<p><span class="font5">Epochs</span></p>
<p><span class="font33">Figure 2.11: An example of overfitting</span></p><h2><a name="bookmark35"></a><span class="font8">2.15 Software Implementations of Neural Networks</span></h2>
<p><span class="font33">There exist many software implementations of neural networks, some of which are open source and other commercial applications. The following implementations were evaluated:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;Joone - Java Object Oriented Neural Engine (open source)</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;OCHRE - Optical Character Recognition (open source)</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;Synapse - by Peltarion (commercial)</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;NeuroDimension - by NeuroSolutions (commercial)</span></p><h2><a name="bookmark36"></a><span class="font8">2.16 Other Software Used</span></h2>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;GUNPlot - It was used to plot all graphs in this report and is an open&nbsp;source project.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;JFreeChart - It is a Java package that allows graphs to be rendered.&nbsp;This Java package was used to generate the graphs in the implemented&nbsp;application.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;GIMP - It is an open source graphics package and was extensively used&nbsp;to manipulate images (rotating, resizing, etc)</span></p><h2><a name="bookmark37"></a><span class="font8">2.17 Summary</span></h2>
<p><span class="font33">This chapter provided an introduction to Optical Music Recognition and a summary of several research projects was given. In this chapter we learnt that OMR is a complicated task due to the incoherency in the typesetting of music.</span></p>
<p><span class="font33">Some of the theoretical concepts that will be used for the implementation of the OpenOMR application have been described and include the use of X- and&nbsp;Y-Pro jections, the Discrete Fourier Transform and Artificial Neural Networks.&nbsp;In the following chapter, we will see how the above concepts are used in the&nbsp;various components of the OMR system.</span></p><h1><span class="font13">Chapter 3</span></h1><h1><a name="bookmark38"></a><span class="font16">Design</span></h1>
<p><span class="font33">This chapter provides a detailed description of the design used to implement the OpenOMR application. A brief overview of the architecture will be described&nbsp;in the section below and an in depth explanation of each component is given in&nbsp;the subsequent sections.</span></p><h2><a name="bookmark39"></a><span class="font8">3.1 Overview</span></h2>
<p><span class="font33">The OMR system developed in this pro ject is based on the O</span><span class="font29"><sup>3</span><span class="font33"></sup>MR discussed in section 2.8.6 architecture, although it differs in several ways. Diagram 3.1 shows&nbsp;an overview of the design used to develop the OMR system for this pro ject.</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Scan Music Score </span><span class="font33">- The printed music score is acquired by means of a&nbsp;scanner at a resolution of 300 dpi (dots per inch) in black and white.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Skew Detection </span><span class="font33">- The Fast Fourier Transform of the image is calculated&nbsp;in order to determine if the staves in the image are skewed. This method&nbsp;will be described following the “stave detection” section as we will first&nbsp;examine why the image needs to be de-skewed.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Stave Detection </span><span class="font33">- The detection of the staves is probably the most critical component of the OMR architecture as all the components following&nbsp;the stave detection heavily rely on knowing the precise location of the&nbsp;staves in the image. The first step in the stave detection algorithm is to&nbsp;determine the thickness of a stave line and the white space between two&nbsp;stave lines. By using those two parameters along with the y-projection of&nbsp;the image, we will be able to detect the staves present in the image.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Level 0 Segmentation </span><span class="font33">- This segmentation phase detects all standalone&nbsp;symbols and filled note heads along with groups of symbols. The segments&nbsp;produced by this level are then used by the next segmentation module to&nbsp;detect the presence of filled note heads. An example of a level 0 segment&nbsp;is shown below:</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-10.jpg" style="width:169pt;height:27pt;"/><img src="MSc Dissertation Report_files/MSc Dissertation Report-11.jpg" style="width:232pt;height:503pt;"/>
<p><span class="font33">Figure 3.1: Architecture Overview</span></p>
<p><span class="font33">5. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Note head Detection </span><span class="font33">- The note head detection module is responsible&nbsp;for analysing the image segments from Level 0 in order to find all filled&nbsp;note heads. If any note heads are found in a Level 0 segment, that segment&nbsp;is labelled as having a node head.</span></p>
<p><span class="font33">6. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Level 1 Segmentation </span><span class="font33">- This module scans through the Level 0 segments that have note heads and splits the image into horizontal segments.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-12.jpg" style="width:211pt;height:69pt;"/>
<p><span class="font33">7. </span><span class="font33" style="font-weight:bold;">Level 2 Segmentation </span><span class="font33">- This module vertically segments the outputs from the Level 1 segmentation module, separating note heads from other&nbsp;features such as slurs, beams, etc... From now on, we will refer to the&nbsp;outputs from the level 2 segmentation module as glyphs. An example of&nbsp;a level 2 segment is shown below.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-13.jpg" style="width:42pt;height:41pt;"/>
<p><span class="font33">8. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Basic Symbol Recognition </span><span class="font33">- This module performs the recognition of&nbsp;the Level 2 glyphs by means of a MLP artificial neural network.</span></p>
<p><span class="font33">9. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Midi File Generation </span><span class="font33">- The pitch and duration for each note is calculated based on the output provided by the neural network and a midi file&nbsp;is generated.</span></p><h2><a name="bookmark40"></a><span class="font8">3.2 Programming Language Choice</span></h2>
<p><span class="font33">The C++ and Java programming languages were taking into consideration as choices to implement this application. Although I was more familiar with the&nbsp;C++ programming language, the Java programming language was chosen over&nbsp;C++ for several reasons. Java's portability feature is an attractive one, especially when working with an open source project. The Java programming&nbsp;language also offers an extensive range of classes which are part of the Standard Developers Kit providing the programmer with flexibility and ease. For&nbsp;example, reading an image into memory and manipulating it is easy to accomplish with the BufferedImage class. Java also has a good support for graphics&nbsp;and sound. The BufferedImage class allows different image file types (“.jpg”,&nbsp;“.bmp”, “.png”, . . . ) to be loaded from disk into memory and manipulated.&nbsp;The BufferedImage class provides methods to read or set individual pixel values&nbsp;int he image.</span></p><h2><a name="bookmark41"></a><span class="font8">3.3 Stave Detection</span></h2>
<p><span class="font33">This section provides a description of the method used to locate the stave(s) present in the image being processed. As most of the subsequent modules&nbsp;rely heavily on knowing the precise location of the staves, this is one of the&nbsp;most critical phases of the OMR architecture. If the stave detection algorithm&nbsp;erroneously detects a stave which isn't one, the subsequent modules will process&nbsp;that as being a stave and the accuracy of the OMR system will be compromised.&nbsp;Similarly, determining the exact coordinates of the stave lines is important as&nbsp;this will be used in the midi generation module to determine the exact pitch of&nbsp;notes.</span></p><h3><a name="bookmark42"></a><span class="font34">3.3.1 Stave Line Parameter Detection</span></h3>
<p><span class="font33">The first step in locating the stave(s) in the music score involves approximating the thickness of each stave line and the white space in between each stave line.&nbsp;Since the staves are the most predominant feature in a music score, a histogram&nbsp;of all consecutive black and white pixel will reveal the thickness of the stave&nbsp;lines and the distance between two stave lines. The method used to determine&nbsp;the two aforementioned parameters is the RLE algorithm mentioned in section&nbsp;2.11.</span></p>
<p><span class="font33">By traversing the music score column by column, we can apply the RLE algorithm and record the number of consecutive black and white pixel runs in&nbsp;two arrays. After applying the RLE algorithm, the arrays contain the number&nbsp;of times a certain amount of consecutive black or white pixels occurred. The&nbsp;size of both arrays was chosen to contain 100 integers for the simple reason that&nbsp;a stave line and the distance between two stave lines is unlikely to exceed 100&nbsp;pixels. The typical thickness of a stave line ranges anywhere from 3-6 pixels&nbsp;and the distance between two stave lines can range from 15 to 30 pixels. This&nbsp;of course depends on the typesetting of the music score [7].</span></p>
<p><span class="font33">We now define a range of lower and upper values for the thickness of the stave lines and the distance between two stave lines as follows:</span></p>
<p><span class="font33">- &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">d1 </span><span class="font33">is&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font29">1&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font33">the&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;&nbsp;&nbsp;&nbsp;found&nbsp;&nbsp;&nbsp;&nbsp;of the distance between two&nbsp;&nbsp;&nbsp;&nbsp;stave&nbsp;&nbsp;&nbsp;&nbsp;lines</span></p>
<p><span class="font33">- &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">d2 </span><span class="font33">is&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font29">3&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font33">the&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;&nbsp;&nbsp;&nbsp;found&nbsp;&nbsp;&nbsp;&nbsp;of the distance between two&nbsp;&nbsp;&nbsp;&nbsp;stave&nbsp;&nbsp;&nbsp;&nbsp;lines</span></p>
<p><span class="font33">- &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">n1 </span><span class="font33">is&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font29">1&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font33">the&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;&nbsp;&nbsp;&nbsp;found&nbsp;&nbsp;&nbsp;&nbsp;for the stave line thickness</span></p>
<p><span class="font33">- &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">n2 </span><span class="font33">is&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font29">3&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font33">the&nbsp;&nbsp;&nbsp;&nbsp;value&nbsp;&nbsp;&nbsp;&nbsp;found&nbsp;&nbsp;&nbsp;&nbsp;for the stave line thickness</span></p>
<p><span class="font33">These parameters will be referred to in the following sections.</span></p>
<p><span class="font33">The RLE algorithm was applied to the image shown in figure 3.3 and the results are shown in figure 3.2. We notice a peak in both graphs. The peak in the graph on the left hand side is the thickness of the stave lines whilst the peak in&nbsp;the graph to the right is the distance between two stave lines. In other words&nbsp;the sequence of 19 consecutive white pixels occurred approximately 9600 times.&nbsp;The sequence of 3 consecutive black pixels occurred approximately 13000 times.&nbsp;Hence we can estimate the stave line thickness to be 3 pixels and the distance&nbsp;between two stave lines to be 19 pixels. Based on this information, we will now&nbsp;be able to proceed to the stave detection, knowing that the height of one stave&nbsp;can be approximated by:</span></p>
<p><span class="font33">H eight = 5d + 4n</span></p>
<p><span class="font33">Where d is the thickness of a stave line and n is the distance between two stave lines. For the example given above, we would expect the height of one stave to&nbsp;be 91 pixels. This is only an approximation as the height of a stave can vary&nbsp;depending on the quality of the scanned music score.</span></p><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-14.jpg" style="width:159pt;height:103pt;"/>
<p><span class="font33">Figure 3.2: Black and White pixel Histogram for “Song for Guy” excerpt by Elton John.</span></p></div><br clear="all"/><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-15.jpg" style="width:172pt;height:103pt;"/></div><br clear="all"/><h3><a name="bookmark43"></a><span class="font34">3.3.2 Stave Detection</span></h3>
<p><span class="font33">In order to detect and locate the position of the staves, we take the y-projection of the image and obtain a one-dimensional array whose contents is the summation of all black pixels along the y-axis.</span></p>
<p><span class="font33">The resulting array is then traversed and all local maximas are found. Having previously calculated the approximate thickness of a stave line and the distance&nbsp;between two stave lines, we can develop an algorithm that will scan the array,&nbsp;looking for five equidistant peaks in range d1 . . . d2 each of approximately the&nbsp;same value. The peaks therefore do not have to have a value above a certain&nbsp;threshold but rather must be within a certain percentage of their neighbouring&nbsp;peaks. This therefore enables staves which may not take up the entire width of&nbsp;the image to be found. Figure 3.4 shows the Y-Pro jection of the image in figure&nbsp;3.3.</span></p><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-16.png" style="width:225pt;height:61pt;"/>
<p><span class="font33">Figure 3.3: Elton John Song</span></p></div><br clear="all"/><img src="MSc Dissertation Report_files/MSc Dissertation Report-17.jpg" style="width:216pt;height:130pt;"/>
<p><span class="font33">Figure 3.4: Y-Projection of figure 3.3</span></p>
<p><span class="font33">The algorithm described above works well when the staves are non-skewed but will however fail if the stave is skewed. Let us imagine a music score which&nbsp;contains exactly one stave with each stave line having a height of exactly 1 pixel.&nbsp;Assuming that the stave is the ONLY feature present in the image (i.e nothing&nbsp;but the stave lines exist in the image), the result of taking the y-pro jection of&nbsp;the image will give us an array with exactly 5 non-zero elements (the peaks)&nbsp;at equidistant locations in the array. If the score is now rotated at an angle&nbsp;and the y-projection of the rotated image is calculated, each stave line will no&nbsp;longer contribute to exactly one element in the array but will rather contribute&nbsp;to several neighbouring elements. We therefore no longer see distinct peaks&nbsp;when a graph of the y-pro jection is plotted. Figure 3.6 is the graph of the y-pro jection of the image in figure 3.5 and we can see that five equidistant lines&nbsp;as seen in figure 3.4 are no longer visible.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-18.jpg" style="width:225pt;height:61pt;"/>
<p><span class="font33">Figure 3.5: Elton John Song Skewed</span></p>
<p><span class="font33">When relying on the y-projection to locate the staves in a music score, it is therefore important to determine the angle by which the staves are skewed.&nbsp;The next section discusses the optional FFT module that is used to detect the&nbsp;that.</span></p>
<p><span class="font33">All staves found in the image are stored in a list. The following information is kept in a data structure which is used in the subsequent modules:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;Top - The uppermost y-coordinate of the stave</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;Bottom - The bottom y-coordinate of the stave</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-19.jpg" style="width:216pt;height:130pt;"/>
<p><span class="font33">Figure 3.6: Elton John Song</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;Left - The leftmost x-coordinate of the stave</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;Right - The rightmost x-coordinate of the stave</span></p><h2><a name="bookmark44"></a><span class="font8">3.4 Skew Correction</span></h2>
<p><span class="font33">This step is optional and will not automatically be performed by the OpenOMR application. The main reason for making it optional is that the DFT is computationally expensive to perform in comparison to the time taken for the other&nbsp;modules to complete their execution. There exists a fast method to approximate the DFT and this is known as the Fast Fourier Transform (FFT). While&nbsp;the Discrete Fourier Transform is computed in O N</span><span class="font29"><sup>2</sup> </span><span class="font33">time, the FFT can be&nbsp;computed in O (N logN) time. Therefore an algorithm to compute the FFT was&nbsp;used in order to improve the efficiency of this component.</span></p>
<p><span class="font33">Figure 3.7 shows the graph of the magnitude components when the Fourier Transform of the image in figure 3.5 is taken. When looking at figure 3.5 we&nbsp;notice that there exists an almost vertical line tilted by a few degrees to the&nbsp;right. From this, the angle by which the staves are rotated can be calculated as&nbsp;mentioned in section 2.13.</span></p>
<p><span class="font33">This method has shown to produce accurate results (which will be discussed in the chapter “results”). There are however some circumstances under which&nbsp;this method will not produce accurate results. It can happen that only a few&nbsp;staves on a page are skewed and the rest are intact. In such circumstances,&nbsp;the method described above may provide an indication as to how much the&nbsp;image needs to be rotated by, but in reality, only a few staves may need to be&nbsp;rotated. Therefore by rotating the image, the staves which were originally non-skewed will now be skewed. In order to avoid this kind of situation, the Fourier&nbsp;Transform of smaller parts of the image can be taken. This is known a windowed&nbsp;Fourier Transform and is described in [3]. The current implementation does not&nbsp;support the windowed Fourier Transform and this is feature which is suggested&nbsp;for a future version of the application and is discussed in chapter 7.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-20.jpg" style="width:257pt;height:257pt;"/>
<p><span class="font33">Figure 3.7: FFT of figure 3.5</span></p><h2><a name="bookmark45"></a><span class="font8">3.5 Image Segmentation (Level 0)</span></h2>
<p><span class="font33">The Level 0 segmentation module attempts to find groups of symbols, individual symbols, and individual note heads and labels them. Groups of symbols are defined as being groups of notes that are attached to one another with beams,&nbsp;or glyphs such as accidentals which are close to one another. Figure 3.8 shows&nbsp;an example of what we would expect a level 0 segment to look like. The main&nbsp;purpose of this module is to reduce the search space of the image in order to&nbsp;reduce the processing time required for the subsequent phases. It is unnecessary&nbsp;to process areas of the image which we are certain contain no musical symbols.</span></p>
<p><span class="font33">The method used to locate groups of symbols relies on the x-projection. The x-projection for each stave is calculated and stored in an array for post processing. We saw above how we could determine the height of a stave and we now&nbsp;introduce an equation that defines the minimum number of black pixels that&nbsp;are required in order for the image segment to have something other than just&nbsp;the stave lines.</span></p>
<p><span class="font33">M inBlackP ixels = 5n</span><span class="font29">2</span></p>
<p><span class="font33">We choose n2 (i.e maximum stave line height) here as we want to try to set a minimum threshold that will allow us to find empty staves. The array&nbsp;containing the x-pro jection is then scanned and as soon as we find values above&nbsp;the minimum threshold, we start a counter. The process then continues until&nbsp;a value below the minimum threshold is found. The counter then defines the&nbsp;width of the new segment that was found and its coordinates are stored in a&nbsp;data structure that is discussed in section 4.1.7.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-21.jpg" style="width:336pt;height:52pt;"/>
<p><span class="font33">Figure 3.8: Sample Level 0 Segmentation</span></p><h2><a name="bookmark46"></a><span class="font8">3.6 Note Head Detection</span></h2>
<p><span class="font33">The purpose of this module is to detect and label any segments containing filled note heads. It does so by analysing the y-projection of the Level 0 segments.</span></p>
<p><span class="font33">A level 0 segment is shown in figure 3.9. In this particular example, we notice that the Level 0 segment contains a total of six note heads. This is reflected in the graph on the right of the image, where we can see six peaks. It should&nbsp;also be noted that the height of the peaks relates to the y-coordinate of the&nbsp;position of the note in the level 0 segment.</span></p>
<p><span class="font33">At this stage, the stems of the notes are also located. All filled note heads have a stem attached to them and the only reason a stem would not be present&nbsp;is if the scan of the music score is of poor quality. Stems are a distinct feature&nbsp;in an image as they are fairly tall in comparison to the height of the note head.&nbsp;By using those principals, we can scan the portion of the image in which a&nbsp;note head was found and determine if the stem is to the left or to the right of&nbsp;the note. In section 3.15, we will see that in order to determine the duration of&nbsp;the note, it is important to know on which side of the note the stem is positioned.</span></p>
<p><span class="font33">Although the current implementation of the note head detection algorithm is able to locate the stem of a note, this information is not used an extra check to&nbsp;make sure that the algorithm correctly detected a note head. As we will see in&nbsp;chapter 6, some notes are falsely detected and in order to minimise the number&nbsp;of false negatives, we could require the note head detection algorithm to find a&nbsp;stem to the left or to the right of it in order to classify it as being a note head.</span></p>
<p><span class="font33">All note heads which were found during this segmentation process along with the position of the stem (left or right) are stored in a data structure which is</span></p>
<p><span class="font33">contained within the level 0 segment data structure. Additionally, a special flag</span></p>
<p><span class="font33">“note_found” is set in the level 0 segment.</span></p><h2><a name="bookmark47"></a><span class="font8">3.7 Symbol Segmentation (Level 1)</span></h2>
<p><span class="font33">In the current implementation, the Level 1 segmentation module will only process Level 0 segments that have been labelled as having a note head. The idea</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-22.jpg" style="width:258pt;height:87pt;"/>
<p><span class="font33">Figure 3.9: Level 0 segment and its corresponding graph after the note head detection algorithm is applied.</span></p>
<p><span class="font33">behind this segmentation level is to separate symbols vertically. That is, we would expect this module to produce new level 1 segments as shown in figure&nbsp;3.10. The algorithm used for this segmentation module is described in section&nbsp;4.1.7.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-23.jpg" style="width:199pt;height:90pt;"/>
<p><span class="font33">Figure 3.10: L1 Segment</span></p><h2><a name="bookmark48"></a><span class="font8">3.8 Note Processing (Level 2)</span></h2><img src="MSc Dissertation Report_files/MSc Dissertation Report-24.jpg" style="width:324pt;height:90pt;"/>
<p><span class="font33">Figure 3.11: Y-Pro jection of Level 1 segment</span></p>
<p><span class="font33">The goal of this module is to separate note heads from other symbols. The level 1 segmentation module has separated the symbols vertically and this module now horizontally segments each of the level 1 segments. This is done by&nbsp;taking the y-pro jection of the image and applying two filters.</span></p>
<p><span class="font33">1. Stem removal - The stem is removed from the y-projection of the level&nbsp;1 segment. Since we know the approximate width of the stem, we can</span></p>
<p><span class="font33">traverse the y-projection in search of a sequence of values within that range and reset them (i.e set them to 0).</span></p>
<p><span class="font33">2. Stave line removal - The stave lines are removed from the projection. We&nbsp;know the approximate coordinates for all five stave lines and we can make&nbsp;use of that knowledge to remove them from the projection.</span></p>
<p><span class="font33">Figure 3.11 shows the segmentation process. A level 1 segment is input into the level 2 segmentation module and the first filter (stem removal) is applied to&nbsp;the y-projection. The y-projection is then passed through a second filter which&nbsp;removes the stave lines. The graph to the left shows the result after the first&nbsp;filter is applied and the graph to the right shows the result after the second&nbsp;filter is applied.</span></p>
<p><span class="font33">Having applied these two filters, we can now search for glyphs in the projection. Any sequence of non-zero values in the projection which are separated by less than D1 consecutive 0 values in the projection are considered to be one&nbsp;glyph. This is an experimental value that was chosen based on the following:&nbsp;glyphs which are attached together such as beams do not appear to be separated by more than the distance between two stave lines. This is of course a&nbsp;parameter which can be varied for experimental purposes .</span></p>
<p><span class="font33">The relevant coordinates of each glyph found are then stored for processing by the neural network.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-25.jpg" style="width:104pt;height:102pt;"/>
<p><span class="font33">Figure 3.12: L2 Segment</span></p><h2><a name="bookmark49"></a><span class="font8">3.9 Neural Network</span></h2>
<p><span class="font33">Several design factors had to be taken into consideration when designing and choosing the type of neural network that would be used for this project. Two&nbsp;types of neural networks were considered and are listed below:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;Multi-Layer Perceptrons (MLP) Backpropagation Neural Network</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;Self Organising Feature Map (SOFM) Neural Network</span></p>
<p><span class="font33">As we saw in section 2.14.1, the MLP network can be trained in super-vised/unsupervised mode whilst the Kohonen network can only be trained in unsupervised mode. We also noted that the Kohonen network has one output&nbsp;node per class of data and only one of those output nodes fires when the network&nbsp;is interrogated. In contrast, the MLP network will output a percent confidence&nbsp;for each output node when interrogated and this is one of the main reasons&nbsp;for choosing the MLP network for the design of our application. The percent&nbsp;confidence of the output nodes is used in the current implementation of the&nbsp;application. If the percent confidence of the classified symbol is below a certain&nbsp;percentage, it is rejected. A future implementation could use this information&nbsp;to then investigate what the supposedly recognised symbol might be.</span></p><h3><a name="bookmark90"></a><span class="font34">3.9.1 Neural Network Design Considerations and Choices</span></h3>
<p><span class="font33">As the implementation of a neural network was outside the scope of this project, an existing implementation of a neural network was used. The neural network&nbsp;software implementations the were surveyed in section 2.15 were tested and the&nbsp;commercial versions were not used as this project was to become an open source&nbsp;one.</span></p>
<p><span class="font33">In order to test each of those neural networks to determine which implementation would be best for our application, training data had to be gathered. In our case, the training data was composed of musical glyphs coming from different&nbsp;music scores. Those were collected by cutting out hundreds of different glyphs&nbsp;and saving them into separate files. In chapter 7 we will discuss how this can&nbsp;be avoided by having our application work semi-automatically. In chapter 6, we&nbsp;discuss which types of symbols and how many of each symbol was used to train&nbsp;the neural network.</span></p>
<p><span class="font33">The neural network engine from the OCHRE applet was taken and by providing several modifications our data was trained with that neural network engine. Encouraging results were obtained but this implementation suffered a flaw in&nbsp;the sense that it did not validate the training data. As we discussed in section&nbsp;3.9, it is extremely important to train a neural network with a training data&nbsp;set and a validation data set in order to avoid ”overfitting”. Albeit working&nbsp;on our training set, it was impossible to detect when overfitting was occurring&nbsp;and the OCHRE neural network engine was dropped from the OpenOMR application. The Joone implementation was tested again and with persistence, it&nbsp;worked (and also provided a data validation feature when training the network).</span></p>
<p><span class="font33">We will now discuss the architecture of the neural network that was chosen and how the data is presented to the neural network. The number of inputs&nbsp;chosen for the neural network reflects the size of the normalised image and 128&nbsp;inputs were chosen. This is a parameter which can of course be varied but it was&nbsp;chosen as the O</span><span class="font29"><sup>3</span><span class="font33"></sup>M R architecture uses a 8x16 (128 values) normalised image&nbsp;as its input to the neural network. It was chosen to have one output node per&nbsp;glyph category.</span></p><h3><a name="bookmark50"></a><span class="font34">3.9.2 Normalising Image Segments</span></h3>
<p><span class="font33">The image segments produced in the Level 2 segmentation phase will have different sizes and with a fixed amount of input nodes in our neural network, we must normalise (or downsample) each image segment to reflect the amount of&nbsp;input nodes from our neural network. In our case, one pixel per input node is</span></p>
<p><span class="font2" style="font-weight:bold;">Original QlypH &nbsp;&nbsp;&nbsp;Scaled Glyph</span></p>
<p><span class="font2" style="font-weight:bold;">58x160 &nbsp;&nbsp;&nbsp;<sub>8</sub>x16</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-26.jpg" style="width:169pt;height:99pt;"/>
<p><span class="font33">Figure 3.13: Normalising a Treble Glyph</span></p>
<p><span class="font33">used. The image can be normalised by selecting a scaling algorithm depending on the quality of the normalised image we require. As will be seen in section&nbsp;4.1.1 a scaling algorithm that is available in the </span><span class="font33" style="font-weight:bold;">Image </span><span class="font33">class of the Java SDK&nbsp;was used. The reason for normalising the level 2 segments is that the inputs to&nbsp;the neural network are fixed. Figure 3.13 shows a treble clef glyph segment and&nbsp;its corresponding normalised image (8 by 16).</span></p>
<p><span class="font33">Figure 3.14 shows how the image is input into the neural network. The image is represented as a two-dimensional array. In order to input the image into the neural network, we can convert the 8 by 16 image into a single array by&nbsp;concatenating each column of the image one after another. Images can be represented in several colour models of which include the RGB and HSV (or HSB)&nbsp;model. The RGB model represents each pixel in the image as a Red, Green and&nbsp;Blue value whilst the HSV represents each pixel as a Hue, Saturation and Value.&nbsp;The Value of the HSV colour model represents the brightness of the pixel and&nbsp;this is what we use as our input into each node of the neural network.</span></p><h2><a name="bookmark51"></a><span class="font8">3.10 Midi Music Generation</span></h2>
<p><span class="font33">The goal of this module is to generate a midi file which can be played through the computer speakers. In essence, this module takes all the level 2 segments&nbsp;that were produced and constructs a midi representation of them based on some&nbsp;rules. We are concerned with finding the pitch and duration for each note. The&nbsp;pitch for each note is relatively simple to find as we already know their x and y&nbsp;coordinates from the note head detection module. We now need to determine&nbsp;the duration of the note and this is done based on which side of the note the&nbsp;stem is located. If the stem is located on the left of the note, we need to look&nbsp;at the glyph directly below the note and if the stem is on the right, we need&nbsp;to look at the segment to the top right of the node. This follows the way that&nbsp;music scores are written in general.</span></p>
<p><span class="font33">Assuming that our neural network would output the string “quaver_line” when it recognises a quaver beam and “crotchet” when it recognises a crotchet, we&nbsp;would expect the following sequence of strings to be output when the score in&nbsp;figure 3.15 is processed.</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-27.jpg" style="width:216pt;height:259pt;"/>
<p><span class="font4">quaver_line</span></p>
<p><span class="font4">crotchet</span></p>
<p><span class="font4">quaver_line</span></p>
<p><span class="font4">quaver_line</span></p>
<p><span class="font4">crotchet</span></p>
<p><span class="font4">quaver_line</span></p>
<p><span class="font4">quaver_line</span></p>
<p><span class="font4">crotchet</span></p>
<p><span class="font4">quaver_line</span></p>
<p><span class="font4">sharp</span></p>
<p><span class="font4">quaver_line</span></p>
<p><span class="font4">crotchet</span></p><h2><a name="bookmark52"></a><span class="font8">3.11 Summary</span></h2>
<p><span class="font33">In this chapter, we discussed the different modules used for the design of the OpenOMR application. We saw how the staves are detected by taking the Y-Pro jection of the image and looking for five equidistant peaks in the pro jection.&nbsp;When staves are skewed, the staves are impossible to locate and we therefore&nbsp;introduced the FFT module that is able to detect the angle by which staves&nbsp;are skewed. The Level 0 segmentation module was then introduced and its&nbsp;main objective is to isolate groups of symbols. We then saw how note heads&nbsp;are detected by using a custom a combination of the RLE algorithm and y-projections. Once all note heads are detected, the level 0 segments are passed&nbsp;to the level 1 segmentation module which isolates symbols vertically. The last</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-28.jpg" style="width:203pt;height:171pt;"/>
<p><span class="font33">phase of the segmentation process is the level 2 segmentation and it separates glyphs in the horizontal direction. The segmentation phase is complete after&nbsp;level 2, and the level 2 segment is input into the neural network for recognition.&nbsp;Finally, a midi file is constructed from all the level 2 segments.</span></p><h1><span class="font13">Chapter 4</span></h1><h1><a name="bookmark53"></a><span class="font16">Implementation</span></h1>
<p><span class="font33">This chapter discusses several considerations taken into account when the OpenOMR system was implemented. Notably justifications as to why the Java programming language was used are provided. A detailed description of the Java package structure of the project is given and in some cases, examples of using those&nbsp;classes are provided.</span></p>
<p><span class="font33">Throughout the implementation phase of this project, it was important to con</span></p>
<p><span class="font33">sider the future development of this application. As one of the main goals was</span></p>
<p><span class="font33">to turn this project into an open source one, it was crucial to keep the structure</span></p>
<p><span class="font33">organised in such a way that will be understandable by others who wish to help</span></p>
<p><span class="font33">develop the OpenOMR application.</span></p>
<p><span class="font33">The package structure was designed in such a way that individual components could be altered or changed if required (neural network, FFT implementation, . . . ). For example, the OCHRE neural network implementation was replaced by the Joone neural network engine. This shows that with a little&nbsp;effort, the whole neural network engine was replaced.</span></p><h2><a name="bookmark54"></a><span class="font8">4.1 Package Structure</span></h2>
<p><span class="font33">As an open source pro ject was created on sourceforge.net, a name for the</span></p>
<p><span class="font33">open source project had to be chosen and the project was named </span><span class="font33" style="font-weight:bold;">openOMR</span><span class="font33">.</span></p>
<p><span class="font33">Therefore the package structure of the OpenOMR application takes the form:</span></p>
<p><span class="font33">openomr.packagename. The package layout is shown in figure 4.1.</span></p><h3><a name="bookmark55"></a><span class="font34">4.1.1 The openomr.ann Package</span></h3>
<p><span class="font33">This section discusses the implementation of the artificial neural network and</span></p>
<p><span class="font33">how it is used. As mentioned in section 3.9, the JOONE implementation was</span></p>
<p><span class="font33">used.</span></p>
<p><span class="font33" style="font-weight:bold;">Data Organisation</span></p>
<p><span class="font33">It was important to structure the way in which the training, validation and testing data were stored. A directory structure to store the data sets was devised&nbsp;and is shown in figure 4.2.</span></p>
<p><span class="font3">openomr</span></p>
<p><span class="font2">-&gt; ann</span></p>
<p><span class="font2">-&gt; data_analysis</span></p>
<p><span class="font3">-► fft</span></p>
<p><span class="font2">-► gui</span></p>
<p><span class="font2">-&gt; imageprocessing</span></p>
<p><span class="font2">-► midi</span></p>
<p><span class="font2">-► omr_engine</span></p>
<p><span class="font33">Figure 4.1: Package layout of OpenOMR</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-29.jpg" style="width:353pt;height:139pt;"/>
<p><span class="font33">Figure 4.2: Directory structure of the ANN files</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">NeuralNetwork.ann </span><span class="font33">- This file contains the saved state of the neural&nbsp;network after the training phase has been completed. Every time the&nbsp;OpenOMR application is loaded, it looks for this file in order to restore&nbsp;the saved state of the neural network.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">list.txt </span><span class="font33">- This file contains the list of glyphs that the neural network has&nbsp;been trained to recognise. This is simply a text file and each line contains&nbsp;the name of a glyph terminated by the end-of-line marker.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">training </span><span class="font33">- This is a directory which contains all the image files that&nbsp;the neural network should use when in training mode. It does not matter how the images are arranged in this directory (i.e sub-directories are&nbsp;allowed) but the file name is what matters. Each file name must begin with one of the lines from the </span><span class="font33" style="font-weight:bold;">lists.txt </span><span class="font33">file in order to be processed&nbsp;when the neural network is trained. For example, if the file </span><span class="font33" style="font-weight:bold;">lists.txt </span><span class="font33">contains a line “crotchet”, then a legal name would be “crotchet1.png” but&nbsp;“1crotchet.png” would be illegal.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">validation </span><span class="font33">- This is a directory which contains all the image files of the&nbsp;different glyphs that have been selected for the validation set when training&nbsp;the neural network. The same rules as discussed for the training directory&nbsp;apply for the structure of this directory.</span></p>
<p><span class="font33">5. </span><span class="font33" style="font-weight:bold;">testing </span><span class="font33">- This is a directory which contains all the image files of the&nbsp;different glyphs that have been selected as the test set. The same rules as&nbsp;discussed for the training directory apply for the structure of this directory.</span></p>
<p><span class="font33" style="font-weight:bold;">Classes</span></p>
<p><span class="font33">This openomr.ann package consists of the following classes:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Trainer </span><span class="font33">- This class is responsible for creating and training the neural&nbsp;network.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">DataPrepare </span><span class="font33">- This class has a set of methods that are used to prepare&nbsp;the data with which the neural network is to be trained. Tasks such as&nbsp;normalising an image are performed within this class.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Interrogator </span><span class="font33">- This class is designed to interrogate the neural network.</span></p>
<p><span class="font33">That is, it can load the state of the neural network (NeuralNetwork.ann) if its not already loaded in memory and pass a normalised image into the&nbsp;network that needs to be classified.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">BatchModeTester </span><span class="font33">- This class will normalise and feed all the images&nbsp;present in the </span><span class="font33" style="font-weight:bold;">testing </span><span class="font33">directory and produce an output file. Each line of&nbsp;this output file contains:</span></p>
<p><span class="font33">(a) &nbsp;&nbsp;&nbsp;The name of the glyph as found in the </span><span class="font33" style="font-weight:bold;">testing </span><span class="font33">directory</span></p>
<p><span class="font33">(b) &nbsp;&nbsp;&nbsp;The name of the glyph as classified by the neural network</span></p>
<p><span class="font33">(c) &nbsp;&nbsp;&nbsp;The percent confidence of the neural network&nbsp;</span><span class="font33" style="font-weight:bold;">Training the Neural Network</span></p>
<p><span class="font33">We will now look at how the data is input into the neural network. In order to train the neural network, we have to provide it with training and validation&nbsp;data sets (in our case, images). The images are read from the ‘neuralNetwork'&nbsp;directory and the </span><span class="font4">getScaledInstance()</span><span class="font33">method of the Java </span><span class="font33" style="font-weight:bold;">Image </span><span class="font33">class is used&nbsp;to scale each image to an 8 pixel wide by 16 pixel high image.</span></p>
<p><span class="font33">The following two arrays must be created for both the training and validation sets.</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;Input training data - This is a two-dimensional array with 128 columns</span></p>
<p><span class="font33">(for each pixel value of in the image) and amountSymbols rows. amountSymbols specifies the number of images that were found in the training or validation&nbsp;directories. Each column is formed by reading an image into memory and&nbsp;converting the two-dimensional representation of the image into a onedimensional representation. The greyscale intensity component of each&nbsp;pixel is used to fill the arrays.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;Desired training data - This is a two-dimensional array with numSymbols&nbsp;columns and amountSymbols rows where numSymbols is the amount of&nbsp;different symbol classes we are training or validating the neural network&nbsp;with.</span></p><div>
<p><span class="font9">Input Training Data</span></p></div><br clear="all"/><div>
<p><span class="font9">Desired Training Data</span></p></div><br clear="all"/><div>
<p><span class="font28">0 &nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;126&nbsp;&nbsp;&nbsp;&nbsp;127</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font8">A ►</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font8">B ►</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font8">A ’</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td>
<p></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td></tr>
<tr><td>
<p></p></td></tr>
<tr><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">D ►</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td>
<p></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td>
<p></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font8">C &gt;</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font8">C ►</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td>
<p></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td></tr>
<tr><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td>
<p></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td>
<p><span class="font8">D—-</span></p></td><td>
<p></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font8">B ►</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td>
<p></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td>
<p></p></td></tr>
</table></div><br clear="all"/><div>
<p><span class="font1" style="font-weight:bold;">0 &nbsp;&nbsp;&nbsp;12&nbsp;&nbsp;&nbsp;&nbsp;3</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font8">A ►</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">B ►</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">A ►</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">D -</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">C ►</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">C ►</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">D &gt;</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">B -</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">0</span></p></td></tr>
</table></div><br clear="all"/>
<p><span class="font33">Figure 4.3: A representation of the input training data and desired training data arrays</span></p>
<p><span class="font33">Figure 4.3 depicts the layout of the input training data and desired training data arrays.</span></p>
<p><span class="font33">When the network is interrogated in order to classify a glyph, it will output a one-dimensional array of type double with amountS ymbols elements. That&nbsp;array contains the percent confidence values for each symbol class. That array&nbsp;is then parsed, and the value with the highest percent confidence is taken to be&nbsp;the glyph classified by the neural network.</span></p><h3><a name="bookmark56"></a><span class="font34">4.1.2 openomr.data analysis</span></h3>
<p><span class="font33">This package is mainly used to either generate graphs or save data in a text format. These classes are mainly intended for developers who wish to either save&nbsp;or view data in a graphical format since we extensively use x and y-pro jections&nbsp;and it can be useful to graphically view them. It consists of the following two&nbsp;classes:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">GNUPlotGenerator </span><span class="font33">- This class will generate a text file in a format that&nbsp;will understood by the GNU Plot program in order to generate charts. Its&nbsp;constructor and method are provided below:</span></p>
<p><span class="font33">(a) &nbsp;&nbsp;&nbsp;</span><span class="font4">GNUPlotGenerator(String fname, int data[], int size)</span></p>
<p><span class="font33">(b) &nbsp;&nbsp;&nbsp;</span><span class="font4">void generateFile()</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">XYChart </span><span class="font33">- This class will render a chart in the form of a BufferedImage.&nbsp;This is convenient from a developer's point of view as charts can easily&nbsp;be generated and displayed in a GUI. Its constructor and method are as&nbsp;follows:</span></p>
<p><span class="font33">(a) &nbsp;&nbsp;&nbsp;</span><span class="font4">XYChart(name, int data[], int size)</span></p>
<p><span class="font33">(b) &nbsp;&nbsp;&nbsp;</span><span class="font4">BufferedImage getChart(int width, int height)</span></p>
<p><span class="font33">- where width and height specify the size size of the image to be rendered.</span></p><h3><a name="bookmark57"></a><span class="font34">4.1.3 openomr.fft</span></h3>
<p><span class="font33">This package contains the </span><span class="font33" style="font-weight:bold;">FFT </span><span class="font33">class which calculates the Fast Fourier Transform of a BufferedImage. This class was originally developed in C++ by Stephen Murrell at the University of Miami and we converted it to Java. Its constructor&nbsp;and public methods are as follows:</span></p>
<p><span class="font33">1. </span><span class="font4">public FFT(BufferedImage buffImage, int size)</span></p>
<p><span class="font33">2. </span><span class="font4">public void doFFT()</span></p>
<p><span class="font33">3. </span><span class="font4">public double getRotationAngle()</span></p>
<p><span class="font33">4. </span><span class="font4">public BufferedImage getImage()</span></p>
<p><span class="font33">This class can be used as follows:</span></p>
<p><span class="font4">FFT fft = new FFT(buffImage, 2048);</span></p>
<p><span class="font4">fft.doFFT();</span></p>
<p><span class="font4">double angleRad = fft.getRotationAngle();</span></p><h3><a name="bookmark58"></a><span class="font34">4.1.4 openomr.gui</span></h3>
<p><span class="font33">An extensive graphical user interface was developed for several reasons. From a developers point of view, a graphical user interface is invaluable in order to&nbsp;test new functionalities. It provides a quick way to test new features on several&nbsp;music scores without having to waste time reloading the whole application each&nbsp;time.</span></p>
<p><span class="font33">The graphical user interface was very useful when checking that the segmentation was properly done, that the staves were being correctly recognised and that the note heads were being properly recognised.</span></p>
<p><span class="font33">From a users point of view, a graphical user interface provides a certain level of comfort especially for novice users. The implementation of the GUI was&nbsp;programmed with Swing and new features/functionalities can easily be added&nbsp;or changed.</span></p>
<p><span class="font33">Screen shots of the GUI are provided in the user manual in Appendix A.</span></p><h3><a name="bookmark59"></a><span class="font34">4.1.5 openomr.imageprocessing</span></h3>
<p><span class="font33">This package contains two classes, one of which is used to rotate an image and the other to create and copy a new buffered image.</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Rotatelmage </span><span class="font33">- This class is used by the openomr.FFT package to rotate&nbsp;an image when correcting skewed staves.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">CopyImage </span><span class="font33">- This class is used to copy a BufferedImage to a new one.</span></p><h3><a name="bookmark60"></a><span class="font34">4.1.6 openomr.midi</span></h3>
<p><span class="font33">This package contains the class that is responsible for generating the midi file. At present, we are limited to representing monophonic scores and after describing&nbsp;how this class works, we will show how it can be extended to provide support&nbsp;for polyphonic scores.</span></p>
<p><span class="font33">1. </span><span class="font4">public MidiGenerator(int key, int tempo, int resolution)</span></p>
<p><span class="font33">2. </span><span class="font4">public void start()</span></p>
<p><span class="font33">- &nbsp;&nbsp;&nbsp;This method will play the midi representation of the file that was created&nbsp;through the computer speakers.</span></p>
<p><span class="font33">3. </span><span class="font4">public void add(int note)</span></p>
<p><span class="font33">- &nbsp;&nbsp;&nbsp;Add a note with pitch </span><span class="font4">note</span><span class="font33">with a duration of 1 beat to the current track</span></p>
<p><span class="font33">4. </span><span class="font4">public void add(int note, int length)</span></p>
<p><span class="font33">- &nbsp;&nbsp;&nbsp;Add a note with pitch </span><span class="font4">note</span><span class="font33">and with a duration of </span><span class="font4">length</span><span class="font33">beats to the&nbsp;current track</span></p>
<p><span class="font33">5. </span><span class="font4">public void addSilence(int length)</span></p>
<p><span class="font33">- &nbsp;&nbsp;&nbsp;Add a rest of duration </span><span class="font4">length</span><span class="font33">to the current track</span></p>
<p><span class="font33">A midi representation of a scale could then be created and played through the computer speakers as follows:</span></p>
<p><span class="font4">MidiGenerator midi = new MidiGenerator(60, 30, 8);</span></p>
<p><span class="font4">midi.add(62);</span></p>
<p><span class="font4">midi.addSilence(1);</span></p>
<p><span class="font4">midi.add(64);</span></p>
<p><span class="font4">midi.addSilence(1);</span></p>
<p><span class="font4">midi.add(65);</span></p>
<p><span class="font4">midi.addSilence(73);</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-30.jpg" style="width:250pt;height:120pt;"/>
<p><span class="font33">Figure 4.4: Midi representation of 'C' scale with two rests represented by '-'</span></p>
<p><span class="font33">We will now briefly look at how we could extend the void add(int note) method in order to add 2 notes to the midi event. The void add(int note)&nbsp;method is defined as follows:</span></p>
<p><span class="font4">private void addStartEvent(int note) throws InvalidMidiDataException {</span></p>
<p><span class="font4">ShortMessage message = new ShortMessage(); message.setMessage(ShortMessage.NOTE_ON, 0, note, volume());&nbsp;track.add(new MidiEvent(message, pos));</span></p>
<p><span class="font4">}</span></p>
<p><span class="font33">The midi representation of the song is stored in a track. A ShortMessage object allows us to create notes and silences in a midi track. If we wanted to&nbsp;add two notes for the same beat, we could modify the above method as follows:</span></p>
<p><span class="font4">private void addStartEvent(int note1, int note2) throws InvalidMidiDataException {</span></p>
<p><span class="font4">ShortMessage message = new ShortMessage(); message.setMessage(ShortMessage.NOTE_ON, 0, note1, volume());&nbsp;track.add(new MidiEvent(message, pos));</span></p>
<p><span class="font4">message.setMessage(ShortMessage.NOTE_ON, 0, note2, volume()); track.add(new MidiEvent(message, pos));</span></p>
<p><span class="font4">}</span></p>
<p><span class="font33">We are now able to play two notes at the same time. This shows that the midi file generator can be extended to provide a richer set of features. At present&nbsp;time, there is no support to save the midi representation of the song to a file&nbsp;but this is a suggested feature for a future version of this application and is&nbsp;discussed in chapter 7.</span></p><h3><a name="bookmark61"></a><span class="font34">4.1.7 openomr.omr .engine</span></h3>
<p><span class="font33">This package contains the core OMR engine that is used to recognise a music score. The most important classes that are part of this package are described&nbsp;below.</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">XProjection </span><span class="font33">- This class calculates the x-projection of a BufferedImage.</span></p>
<p><span class="font33">It is currently being used for the development of a more reliable note head detection algorithm which will be present in a future release of this&nbsp;application and is discussed in chapter 7.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">YProjection </span><span class="font33">- This class calculates the y-projection of a BufferedImage.</span></p>
<p><span class="font33">It is currently used by the stave detection, note head detection and level 2 segmentation algorithms. The public methods available are:</span></p>
<p><span class="font33">(a) &nbsp;&nbsp;&nbsp;</span><span class="font4">void calcYProjection(int startH, int endH, int startW, int endW)</span></p>
<p><span class="font33">-startH is the starting y-coordinate for the y-projection -endH is the end y-coordinate for the y-projection&nbsp;-startW is the starting x-coordinate for the y-projection&nbsp;-endW is the ending x-coordinate for the y-projection</span></p>
<p><span class="font33">(b) &nbsp;&nbsp;&nbsp;</span><span class="font4">int[ ] getYProjection()</span></p>
<p><span class="font33">This method simply returns the y-projection calculated</span></p>
<p><span class="font33">(c) </span><span class="font4">void print YProjection()</span></p>
<p><span class="font33">This method is intended for debugging purposes and will print the y-projection calculated to standard output.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">StaveParameters </span><span class="font33">- This class is responsible for determining the stave&nbsp;parameters which were discussed in section 3.3.1. Its constructor takes a&nbsp;BufferedImage as a parameter. The following public methods are provided&nbsp;and return the parameters described in section 3.3.1.</span></p>
<table border="1">
<tr><td>
<p><span class="font33">(a)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">int getD1()</span></p></td></tr>
<tr><td>
<p><span class="font33">(b)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">int getD2()</span></p></td></tr>
<tr><td>
<p><span class="font33">(c)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">int getN1()</span></p></td></tr>
<tr><td>
<p><span class="font33">(d)</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">int getN2()</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">StaveDetection</span></p></td></tr>
</table>
<p><span class="font33">This class is responsible for detecting all the stave lines present in an image. Its constructor is given the y-projection of the image and the stave&nbsp;line parameters. It then looks for five equidistant peaks in the projection&nbsp;and stores the coordinates in an object which has the following fields:</span></p><div>
<p><span class="font4">private</span></p>
<p><span class="font4">private</span></p>
<p><span class="font4">private</span></p>
<p><span class="font4">private</span></p>
<p><span class="font4">private</span></p>
<p><span class="font4">private</span></p>
<p><span class="font4">private</span></p></div><br clear="all"/>
<p><span class="font4">LinkedList&lt;L0_Segment&gt; symbolPos;</span></p>
<p><span class="font4">int top;</span></p>
<p><span class="font4">int bottom;</span></p>
<p><span class="font4">int staveNumber;</span></p>
<p><span class="font4">int noteDistance;</span></p>
<p><span class="font4">int start;</span></p>
<p><span class="font4">int end;</span></p>
<p><span class="font33">The top and bottom field store the y-coordinates at which the stave found start and ends. The start and end fields store the starting and ending&nbsp;x-coordinates. When all level 0 segments are found at a later stage, they&nbsp;are stored in a linked list within this object (symbolPos field).</span></p>
<p><span class="font33">The public methods are:</span></p>
<p><span class="font33">(a) </span><span class="font4">void locateStaves()</span></p>
<p><span class="font33">(b) </span><span class="font4">void calcNoteDistance()</span></p>
<p><span class="font33">5. </span><span class="font33" style="font-weight:bold;">L(LSegment</span></p>
<p><span class="font33">This class stores the starting and ending coordinates of the level 0 segments and also indicates whether or not a note is present in that segment. This</span></p>
<p><span class="font33">class also analyses the level 0 segments to find any level 1 segments and for each level 1 segment found, a new level 1 object is created. That new</span></p>
<p><span class="font33">level 1 object is stored in the </span><span class="font33" style="font-weight:bold;">L(LSegment </span><span class="font33">class.</span></p>
<p><span class="font4">public int start;</span></p>
<p><span class="font4">public int stop;</span></p>
<p><span class="font4">public boolean hasNote; \ldots</span></p>
<p><span class="font33">6. </span><span class="font33" style="font-weight:bold;">LI-Segment </span><span class="font33">The algorithm to vertically separate note heads from other&nbsp;symbols works as follows:</span></p>
<p><span class="font33">(a) If a Level 0 segment begins with a note head, create a Level 1 segment&nbsp;which has the height of the level 0 segment and the width of the note&nbsp;head. If no note heads are present at the start of the level 0 segment,&nbsp;create a level 1 segment which has the height of the level 0 segment.&nbsp;The width of this segment will be defined to be the distance from the&nbsp;start of the level 0 segment until the start of the first note head in&nbsp;the level 0 segment.</span></p>
<p><span class="font33">(b) If a level 1 segment containing a note head was created in the previous step, take the ending coordinate of that note head and find the&nbsp;beginning coordinate of the next note head and create a new level 1&nbsp;segment of that width. If the previous level 1 segment did not contain a note, find the next note and create a new level 1 segment with&nbsp;a width of the note head. Repeat this process until the whole width&nbsp;of the level 0 segment has been processed.</span></p>
<p><span class="font33">Of course we can set a threshold so that the segmentation module does not create level 1 segments which would be extremely small (in the region&nbsp;of say 5 pixels). Such small segments would probably not be of much use&nbsp;and we can therefore omit them. A small segment could appear if notes&nbsp;are tightly packed together or if we are processing the end of a segment.</span></p>
<p><span class="font33">This class records the starting and ending coordinates of the level 1 segments. This class also analyses all the level 1 segments in order to produce level 2 segments.</span></p>
<p><span class="font4">private int xStart; private int xStop;</span></p>
<p><span class="font4">\ldots</span></p>
<p><span class="font33">7. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">L2_Segment</span></p>
<p><span class="font33">This class stores information about all Level 2 segments as described in section 3.8. When the Level 2 segment is passed through the neural network, the output from the neural network in terms of symbol name and&nbsp;percent confidence is stored in this object.</span></p>
<p><span class="font4">private int yPosition; private String symbolName;&nbsp;private double accuracy;</span></p>
<p><span class="font33">8. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">NoteHead</span></p>
<p><span class="font33">We can approximate a note head to have a height of 2N 2 + D2 by considering the case when a note head is placed between two stave lines. The process of detecting the note heads is as follows:</span></p>
<p><span class="font33">(a) Process each column of the image one by one.</span></p>
<p><span class="font33">(b) Apply the RLE algorithm to a column and where a run of black pixels exceeds D 2, set all the values for that run to 0.</span></p>
<p><span class="font33">(c) Iterate through the column again and remember the longest run of&nbsp;black pixels and record this in a new array if the value of the run</span></p>
<p><span class="font33">exceeds 2 </span><span class="font4">* </span><span class="font33">N. This ensures that runs which are similar to the height of a note are recorded.</span></p>
<p><span class="font33">(d) &nbsp;&nbsp;&nbsp;Repeat steps 2 and 3 until all columns have been processed.</span></p>
<p><span class="font33">(e) &nbsp;&nbsp;&nbsp;We then apply a simple filter which eliminates any values which are&nbsp;less than </span><span class="font2">DD </span><span class="font33">wide. We are now left with an array which contains sets&nbsp;of peaks. Those peaks correspond to the note heads found by the&nbsp;above steps and we locate those notes by finding the peaks in the&nbsp;array.</span></p>
<p><span class="font33">This class stores information about all the note heads. It stores the x and y coordinates of each note along with the stem position (left=0, right=1).</span></p>
<p><span class="font4">public int x;</span></p>
<p><span class="font4">public int y;</span></p>
<p><span class="font4">public int stemInfo;</span></p>
<p><span class="font33">9. </span><span class="font33" style="font-weight:bold;">PitchCalculation</span></p>
<p><span class="font33">This class calculates the pitch of a note based on the y-coordinate of the note. We know the distance between two notes, the y-coordinate of the&nbsp;last stave line in a stave and the y-coordinate of the centre of the note and&nbsp;given that information, we can calculate the pitch of the note as follows:</span></p><div>
<p><span class="font33">N ote =</span></p></div><br clear="all"/>
<p><span class="font33" style="text-decoration:underline;">refPos </span><span class="font4" style="text-decoration:underline;">- </span><span class="font33" style="text-decoration:underline;">yPos</span></p>
<p><span class="font29">2 </span><span class="font33" style="font-style:italic;">noteDistance</span></p>
<p><span class="font33">Figure 4.5 shows the different parameters required to calculate the pitch of a note.</span></p>
<p><span class="font33">Assuming: </span><span class="font33" style="font-style:italic;">refPos</span><span class="font33"> = 465, </span><span class="font33" style="font-style:italic;">yP os</span><span class="font33"> = 500 and </span><span class="font33" style="font-style:italic;">noteDistance</span><span class="font33"> = 35 we can calculate </span><span class="font33" style="font-style:italic;">N ote</span><span class="font33"> = </span><span class="font4">-</span><span class="font33">2 from which we can determine the pitch of the note&nbsp;as follows:</span></p><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-31.jpg" style="width:159pt;height:135pt;"/>
<p><span class="font33">Figure 4.5: Calculating the pitch of a note</span></p></div><br clear="all"/>
<p><span class="font33">We know that there are 7 notes (‘A'-‘G') and that </span><span class="font33" style="font-style:italic;">refP os</span><span class="font33"> is located at note ‘E'. Therefore given the value of </span><span class="font33" style="font-style:italic;">N ote</span><span class="font33">, we can determine the letter&nbsp;corresponding to the note.</span></p>
<p><span class="font4">if (Note \% 7 == 0)</span></p>
<p><span class="font4">System.out.print(&quot;E &quot;);</span></p>
<p><span class="font4">else if (Note % 7 == 1)</span></p>
<p><span class="font4">System.out.print(&quot;F &quot;);</span></p>
<p><span class="font4">else if (Note % 7 == 2)</span></p>
<p><span class="font4">System.out.print(&quot;G &quot;);</span></p>
<p><span class="font4">else if (Note % 7 == 3)</span></p>
<p><span class="font4">System.out.print(&quot;A &quot;);</span></p>
<p><span class="font4">else if (Note % 7 == 4)</span></p>
<p><span class="font4">System.out.print(&quot;B &quot;);</span></p>
<p><span class="font4">else if (Note % 7 == 5)</span></p>
<p><span class="font4">System.out.print(&quot;C &quot;);</span></p>
<p><span class="font4">else if (Note % 7 == 6)</span></p>
<p><span class="font4">System.out.print(&quot;D &quot;);</span></p>
<p><span class="font33">From our example above, </span><span class="font4">-</span><span class="font33">2 mod 7 = 5 which correctly tells us that the note is a 'C'.</span></p><h2><a name="bookmark62"></a><span class="font8">4.2 Using the OpenOMR Engine</span></h2>
<p><span class="font33">We discussed the individual packages and their functionality within the OpenOMR application. We will now discuss how the core engine of the OpenOMR package could be used to integrate it with another application or as a stand-alone&nbsp;application. One example of a stand-alone application not requiring a GUI is&nbsp;a music recognition web service provided through a website. A music score is&nbsp;uploaded through a website and the OpenOMR engine processes that score at&nbsp;the server side and uploads a Midi file to the user representing the recognised&nbsp;score.</span></p>
<p><span class="font33">The code below shows how an image (assumed to be loaded and stored in a variable buffImage) could be recognised and played through the computer&nbsp;speakers.</span></p>
<p><span class="font4">//Take the y-projection of image</span></p>
<p><span class="font4">YProjection yproj = new YProjection(buffImage);</span></p>
<p><span class="font4">yproj.calcYProjection(0, buffImage.getHeight(), 0, buffImage.getWidth());</span></p>
<p><span class="font4">//Calculate the stave line parameters</span></p>
<p><span class="font4">StaveParameters params = gui.getStaveLineParameters();</span></p>
<p><span class="font4">params.calcParameters();</span></p>
<p><span class="font4">//Find all the staves</span></p>
<p><span class="font4">StaveDetection staveDetection = new StaveDetection(yproj, params);</span></p>
<p><span class="font4">staveDetection.locateStaves();</span></p>
<p><span class="font4">staveDetection.calcNoteDistance();</span></p>
<p><span class="font4">//Segment image (Level0, 1 and 2) and recognise all symbols</span></p>
<p><span class="font4">OMREngine engine = new OMREngine(buffImage, staveDetection, neuralNet);</span></p>
<p><span class="font4">engine.processAll();</span></p>
<p><span class="font33">At this stage, the OMREngine has now processed the entire score and has generated an internal representation of the recognised score. We can now generate and play a midi file as follows:</span></p>
<p><span class="font4">ScoreGenerator midiFile = new ScoreGenerator(staveDetection.getStaveList());</span></p>
<p><span class="font4">midiFile.makeSong(64);</span></p>
<p><span class="font4">midiFile.start();</span></p><h2><a name="bookmark63"></a><span class="font8">4.3 summary</span></h2>
<p><span class="font33">In this chapter we discussed the implementation details of the different Java</span></p>
<p><span class="font33">Packages used for the OpenOMR application. In particular, we payed close</span></p>
<p><span class="font33">attention to the openomr.ann and openomr.omr .engine packages as they are the core of the application. We also identified how the openomr.midi package can&nbsp;be extended to provide polyphonic support in future releases of this application.</span></p><h1><span class="font13">Chapter 5</span></h1><h1><a name="bookmark64"></a><span class="font16">Testing</span></h1>
<p><span class="font33">This chapter will discuss how the overall system was tested and the results will be presented in the next chapter. It is critical to thoroughly test an application&nbsp;in order to identify which components should be improved in future releases.</span></p><h2><a name="bookmark65"></a><span class="font8">5.1 Testing Environment</span></h2>
<p><span class="font33">The application was tested under the WindowsXP, Linux and Mac OS X operating systems. Even though Java programming language was selected due to its cross-platform compatibility, some of the GUI components did not always&nbsp;display as expected on the Mas OS X platform. It was therefore important to&nbsp;test any new GUI features on the three operating systems in order to preserve&nbsp;the cross-compatibility feature of the application.</span></p><h2><a name="bookmark66"></a><span class="font8">5.2 False Positives and False Negatives</span></h2>
<p><span class="font33">“We want to decide if a person will fail as a police officer. So a </span><span class="font33" style="font-weight:bold;">false positive </span><span class="font33">is if we incorrectly say that a person will fail. A </span><span class="font33" style="font-weight:bold;">false negative </span><span class="font33">on the other&nbsp;hand is if we incorrectly predict that person won't fail. ” [22]</span></p>
<p><span class="font33">In terms of evaluating our system:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">A false positive </span><span class="font33">- is for example if the application falsely identifies a&nbsp;note head which isn't one.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">A false negative </span><span class="font33">- is for example when the application fails to detect&nbsp;a note present in the image.</span></p><h3><a name="bookmark67"></a><span class="font34">5.2.1 FFT Module</span></h3>
<p><span class="font33">The FFT module will be tested by taking one score and rotating it at different angles. The score used to test this module will have all its staves perfectly aligned with the horizontal axis. That score will then be rotated in an image&nbsp;editor (such as Gimp) and tested with the FFT module. The case for which&nbsp;only a few staves in the score are rotated will also be simulated.</span></p>
<p><span class="font33">The test cases are as follows:</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;Rotate the original image by 0</span><span class="font2">°</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;Rotate the original image by 0.2</span><span class="font2">°</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;Rotate the original image by 0.5</span><span class="font2"><sup>°</sup></span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;Rotate the original image by 1</span><span class="font2"><sup>°</sup></span></p>
<p><span class="font33">5. &nbsp;&nbsp;&nbsp;Rotate the original image by 2</span><span class="font2"><sup>°</sup></span></p>
<p><span class="font33">6. &nbsp;&nbsp;&nbsp;Only rotate the bottom half of the original image by 0.5</span><span class="font2"><sup>°</sup></span></p>
<p><span class="font33">All of the images above will be input into the FFT module which will calculate the angle of rotation. That will then be compared to the angle at which the original score was rotated by, from which a percentage error can be determined.</span></p><h3><a name="bookmark68"></a><span class="font34">5.2.2 Stave Detection</span></h3>
<p><span class="font33">The simplest way to check that the staves were all correctly identified is to paint them over the recognised score. This module will therefore be visually assessed&nbsp;and if the staves as painted by the OpenOMR application do not appear to&nbsp;completely cover the staves in the original image, then we will reject it as being&nbsp;correctly identified.</span></p><h3><a name="bookmark69"></a><span class="font34">5.2.3 Note Heads Detected</span></h3>
<p><span class="font33">The note heads are painted on top of the original notes as blue square boxes and this will enable us to visually assess how many note heads were correctly&nbsp;and incorrectly identified. We will count the total number of note heads in the&nbsp;original score, the number of false positives and the number of false negatives.&nbsp;This will then allow us to calculate the accuracy of the note head detection&nbsp;algorithm in terms of notes correctly recognised.</span></p><h3><a name="bookmark70"></a><span class="font34">5.2.4 Pitch Calculation</span></h3>
<p><span class="font33">For each note head found, the program outputs a letter 'A' through 'G' based on the calculated pitch of the note head. The pitch for each note head on the&nbsp;original score needs to be manually determined and this can then be compared&nbsp;with the pitch found by the program.</span></p><h3><a name="bookmark71"></a><span class="font34">5.2.5 Note Duration</span></h3>
<p><span class="font33">For each note head found, the program outputs the duration of the note as an integer having a value from ‘1' to ‘3'. A value of ‘1' represents a semi-quaver,&nbsp;a value of '2' represents a quaver and a value of ‘3' represents a crotchet. The&nbsp;output value is then compared with the actual duration of each note head in&nbsp;the original score.</span></p><h3><a name="bookmark72"></a><span class="font34">5.2.6 Neural Network</span></h3>
<p><span class="font33">In order to test and train the neural network, several hundred music glyphs were extracted from a selection of music scores. The glyphs in the 'testing' directory&nbsp;described in section 4.1.1 were used to determine the percent accuracy of the&nbsp;neural network and we want to know how many symbols are correctly and&nbsp;incorrectly classified.</span></p>
<p><span class="font33">Ideally, the glyphs produced from the level 2 segmentation module should be used to determine the overall accuracy of the neural network. However, the&nbsp;level 2 segmentation module is not producing accurate results as is and if the&nbsp;level 2 segmentation module is improved in a future release, this method for&nbsp;testing the neural network should be used.</span></p><h2><a name="bookmark73"></a><span class="font8">5.3 Graphical User Interface Testing</span></h2>
<p><span class="font33">The graphical user interface was tested to in order to determine its stability and reliability. Two common software engineering testing methods were used to test&nbsp;this software and they are the ‘monkey testing' and ‘stress testing'. Monkey&nbsp;testing is a technique for which a user is asked to use the application and find&nbsp;bugs (such as entering a letter when a digit is required). Stress testing is when&nbsp;a program is tested beyond its known operating capabilities.</span></p><h3><a name="bookmark74"></a><span class="font34">5.3.1 Monkey Testing</span></h3>
<p><span class="font33">The monkey testing for this application is to be performed by two students from the Department of Computing at Imperial College. They will thoroughly test&nbsp;the application and make a note of any cases for which the application crashed.</span></p><h3><a name="bookmark75"></a><span class="font34">5.3.2 Stress Testing</span></h3>
<p><span class="font33">We want to determine how well the program will cope if:</span></p>
<p><span class="font33">1. It is given a huge image (greater than 5Mb)</span></p>
<p><span class="font33">2. If no music is present in the image. (For example, the portrait of someone)</span></p><h1><span class="font13">Chapter 6</span></h1><h1><a name="bookmark91"></a><span class="font16">Results</span></h1>
<p><span class="font33">This chapter provides results from the tests that were described in chapter 5.</span></p><h2><a name="bookmark76"></a><span class="font8">6.1 FFT Module</span></h2>
<p><span class="font33">The test cases discussed in section 5.2.1 were performed on the score in figure A.1 and the results are shown below:</span></p>
<table border="1">
<tr><td>
<p><span class="font33">Original</span></p></td><td>
<p><span class="font33">FFT Module</span></p></td><td>
<p><span class="font33">% error</span></p></td></tr>
<tr><td>
<p><span class="font33">0</span><span class="font2">°</span></p></td><td>
<p><span class="font33">0</span><span class="font2">°</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">0</span></p></td></tr>
<tr><td>
<p><span class="font33">0.2</span><span class="font2">°</span></p></td><td>
<p><span class="font33">0.253</span><span class="font2">°</span></p></td><td>
<p><span class="font33">26.5</span></p></td></tr>
<tr><td>
<p><span class="font33">0.5</span><span class="font2">°</span></p></td><td>
<p><span class="font33">0.506</span><span class="font2">°</span></p></td><td style="vertical-align:middle;">
<p><span class="font33">1.2</span></p></td></tr>
<tr><td>
<p><span class="font33">1</span><span class="font2">°</span></p></td><td>
<p><span class="font33">1.0127</span><span class="font2">°</span></p></td><td>
<p><span class="font33">1.27</span></p></td></tr>
<tr><td>
<p><span class="font33">2</span><span class="font2">°</span></p></td><td>
<p><span class="font33">2.099</span><span class="font2">°</span></p></td><td>
<p><span class="font33">4.95</span></p></td></tr>
</table>
<p><span class="font33">Table 6.1: FFT Module Results</span></p>
<p><span class="font33">The total overall error is 6.78% and this error does not include the test case</span></p>
<p><span class="font33">which saw the bottom half of the score rotated by 0.5</span><span class="font2">°</span><span class="font33">. That was ommited from the overall error as the FFT module does not work when only a few staves&nbsp;are rotated. A windowed FFT could overcome this problem and is discussed&nbsp;chapter 7.</span></p><h2><a name="bookmark77"></a><span class="font8">6.2 Stave Detection</span></h2>
<p><span class="font33">The stave detection module was tested on the scores in figures A.1 and A.2. Additionally, the stave detection module was also tested when the score in A.1&nbsp;was rotated by 0.5</span><span class="font2"><sup>°</span><span class="font33"></sup>. The results are shown in figures 6.1, 6.2 and 6.6.</span></p>
<p><span class="font33">Although only two staves are shown in figure 6.1, all staves were correctly recognised for this score. The stave in figure 6.2 was also correctly recognised.&nbsp;However when the the score in figure A.1 was rotated by 0.5</span><span class="font2"><sup>°</span><span class="font33"></sup>, the stave detection&nbsp;algorithm did not work properly and that is seen in figure 6.6.</span></p>
<p><span class="font33">We can therefore conclude from these test cases that when the stave is slightly skewed, the stave accuracy of the stave detection algorithm deteriorates and this shows the important role that the FFT module has in order to&nbsp;accurately identify staves.</span></p><h2><a name="bookmark78"></a><span class="font8">6.3 Note Head Detected</span></h2>
<p><span class="font33">The note head detection module was tested with the scores in figures A.1, A.2 and A.3. Table 6.2 below shows the results obtained after running the note head&nbsp;detection module on those scores.</span></p>
<table border="1">
<tr><td>
<p></p></td><td>
<p><span class="font33"># Note Heads</span></p></td><td>
<p><span class="font33"># Note Heads Found by Module</span></p></td><td>
<p><span class="font33">False</span></p>
<p><span class="font33">Positives</span></p></td><td>
<p><span class="font33">False</span></p>
<p><span class="font33">Negatives</span></p></td></tr>
<tr><td>
<p><span class="font33">Score 1</span></p></td><td>
<p><span class="font33">371</span></p></td><td>
<p><span class="font33">404</span></p></td><td>
<p><span class="font33">38</span></p></td><td>
<p><span class="font33">5</span></p></td></tr>
<tr><td>
<p><span class="font33">Score 2</span></p></td><td>
<p><span class="font33">15</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">0</span></p></td><td>
<p><span class="font33">15</span></p></td></tr>
<tr><td>
<p><span class="font33">Score 3</span></p></td><td>
<p><span class="font33">165</span></p></td><td>
<p><span class="font33">167</span></p></td><td style="vertical-align:middle;">
<p><span class="font33">18</span></p></td><td style="vertical-align:middle;">
<p><span class="font33">16</span></p></td></tr>
</table>
<p><span class="font33">Table 6.2: Note Head Detection Results</span></p>
<p><span class="font33">The results shown in table 6.2 provide encouraging results for the note head detection module.</span></p><h2><a name="bookmark79"></a><span class="font8">6.4 Pitch Calculation</span></h2>
<p><span class="font33">The pitch calculation for the score in figures A.1, A.2 and A.3 are shown in tables 6.3, 6.4 and 6.5 respectively. The top row shows the pitch found by&nbsp;manually going through the score whilst the bottom row shows the pitch as&nbsp;calculated by the application.</span></p>
<table border="1">
<tr><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">C</span></p></td></tr>
<tr><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">C</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font33">G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">E</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">C</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">B</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">D</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">B</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">D</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">B</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">D</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">B</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">G</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">F</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">D</span></p></td></tr>
<tr><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">D</span></p></td></tr>
</table>
<p><span class="font33">Table 6.3: Actual pitch versus Calculated Pitch for figure A.1</span></p>
<table border="1">
<tr><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">C</span></p></td></tr>
<tr><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">C</span></p></td></tr>
</table><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-32.jpg" style="width:371pt;height:96pt;"/>
<p><span class="font33">Figure 6.1: Stave Detection Results for Figure A.1</span></p></div><br clear="all"/>
<p><span class="font27" style="font-weight:bold;">A *C* scale in LilyPond</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font4">-y-</span></p></td><td>
<p><span class="font33">—</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">—mr</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font33" style="font-weight:bold;">zL * * &nbsp;&nbsp;&nbsp;<sup>1</sup></span></p></td><td>
<p><span class="font33"> 1 &gt;</span></p></td><td>
<p><span class="font33">-I </span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">1</span></p></td></tr>
<tr><td>
<p></p></td><td colspan="2">
<p><span class="font33">i </span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font33" style="font-weight:bold;">-S<sup>5</sup>—</span></p></td><td>
<p><span class="font33">*’ 1 1</span></p></td><td>
<p><span class="font33">'1 *'■</span></p></td><td>
<p><span class="font4">’ JJ</span></p></td></tr>
</table>
<p><span class="font33">Figure 6.2: Stave Detection Results for Figure A.2</span></p><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-33.jpg" style="width:370pt;height:80pt;"/>
<p><span class="font33">Figure 6.3: Stave Detection Results for Figure A.1 (rotated)</span></p></div><br clear="all"/><img src="MSc Dissertation Report_files/MSc Dissertation Report-34.jpg" style="width:384pt;height:528pt;"/>
<p><span class="font27" style="font-weight:bold;">A *C* scale in LilyPond</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font10">-y-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font10">—1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font10">—rTir</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font4" style="font-style:italic;">A. ri ]</span></p></td><td>
<p><span class="font10"> 1 -- </span><span class="font4" style="font-style:italic;">9</span></p></td><td>
<p><span class="font10">L </span></p></td><td style="vertical-align:bottom;">
<p><span class="font10">1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font10">rfr\ '<sup>7</sup></span></p></td><td colspan="2">
<p></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font4" style="font-style:italic;">o'-</span></p></td><td>
<p><span class="font10">'1 «'■</span></p></td><td>
<p><span class="font18">Jl</span></p></td></tr>
</table>
<p><span class="font33">Figure 6.5: Note Head Detection Results for Figure A.2</span></p>
<table border="1">
<tr><td colspan="2">
<p><span class="font5">5 __----—</span></p>
<p><span class="font5">,.i</span><span class="font38">---.&quot;r^</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font5">, , ■ - —1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font5">w/</span></p>
<p><span class="font23" style="font-variant:small-caps;">Mwb</span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font5"><sub>/F</sub>&amp; rJTJTni</span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">--p\</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font5">H-=-i</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font3">.....1&quot; &nbsp;&nbsp;&nbsp;</span><span class="font38">'&quot;W”&nbsp;&nbsp;&nbsp;&nbsp;'</span></p></td><td>
<p></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font5">1 in </span><span class="font3">.1 &nbsp;&nbsp;&nbsp;&quot;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font2" style="font-weight:bold;font-style:italic;">at ~</span></p></td><td>
<p><span class="font3">r </span><span class="font5">r</span></p></td><td>
<p><span class="font5">----J—j—</span></p></td><td>
<p><span class="font5">73-™-</span></p></td></tr>
<tr><td>
<p><span class="font5">I V'J &nbsp;&nbsp;&nbsp;—J&nbsp;&nbsp;&nbsp;&nbsp;U q</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font4">1 ’-<sup>r</sup> 1 ■</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font5">11) •</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">1</span></p>
<p><span class="font5">rm &quot;H</span></p></td><td>
<p><span class="font5">*&quot;' ■</span></p>
<p><span class="font5">rm r^r^n</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font4">————————r</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font3">i &nbsp;&nbsp;&nbsp;</span><span class="font3" style="font-style:italic;">j „</span></p></td><td>
<p><span class="font4">1 I </span><span class="font5">J l l l Jl</span></p></td><td>
<p><span class="font5">■ ' -. J .:. &nbsp;&nbsp;&nbsp;</span><span class="font3">j</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font1" style="font-weight:bold;font-style:italic;font-variant:small-caps;">lJ '-&gt; j</span><span class="font2" style="font-weight:bold;font-style:italic;"> &nbsp;&nbsp;&nbsp;a&nbsp;&nbsp;&nbsp;&nbsp;a</span></p></td><td>
<p><span class="font5">J U<sup>1</sup> </span><span class="font0" style="font-weight:bold;font-style:italic;font-variant:small-caps;">lJ</span><span class="font4" style="font-weight:bold;font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;jv-j- -</span></p></td><td>
<p><span class="font5">fl  “ 4 .. &nbsp;&nbsp;&nbsp;</span><span class="font3">1</span></p></td></tr>
<tr><td>
<p><span class="font5">'<sup>Lr</sup> &quot; ^2 1 ’</span></p></td><td>
<p><span class="font3" style="font-style:italic;">J &nbsp;&nbsp;&nbsp;j&nbsp;&nbsp;&nbsp;&nbsp;...</span></p></td><td>
<p></p></td><td>
<p><span class="font5">r—- j</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font29">5  —-———</span></p>
<p><span class="font17">J ---</span><span class="font10" style="font-weight:bold;">T&quot;n</span></p></td><td>
<p></p></td><td colspan="2">
<p><span class="font29">5 _____</span></p></td></tr>
<tr><td>
<p><span class="font17">...... <sup>J</sup>JJ73</span></p>
<p><span class="font15"><sup>?</sup> JJJl JJJ 1</span></p></td><td>
<p><span class="font29">•J &nbsp;&nbsp;&nbsp;</span><span class="font35" style="font-style:italic;">I-</span></p>
<p><span class="font15">JJJJ</span></p></td><td>
<p><span class="font20">--</span></p>
<p><span class="font20">W»</span></p></td><td style="vertical-align:bottom;">
<p><span class="font15">pw</span></p></td></tr>
</table>
<table border="1">
<tr><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font3">1 4 0---,</span></p>
<p><span class="font3">1- ■.' Wi</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3"><sub>r</sub>4-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">3</span></p>
<p><span class="font3">1 ‘j &nbsp;&nbsp;&nbsp;</span><span class="font7">-t </span><span class="font15">-n</span></p></td></tr>
<tr><td>
<p><span class="font3">&gt;.f &nbsp;&nbsp;&nbsp;</span><span class="font15">jl/'</span></p></td><td>
<p></p></td><td>
<p><span class="font19">* &gt;</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font3">L<sub>&gt;:</sub> &nbsp;&nbsp;&nbsp;- .</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font3">— — <sup>=</sup>--p--</span></p></td><td>
<p><span class="font33" style="font-style:italic;">f</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3"><sup>5</sup> 2 1</span></p></td><td>
<p><span class="font3">JJ. j</span></p>
<p><span class="font30" style="font-style:italic;">5~2l</span></p></td><td>
<p><span class="font3">^2 1 J:</span></p>
<p><span class="font3">5</span></p></td><td>
<p><span class="font3">J ~ <sup>U</sup></span></p></td></tr>
<tr><td colspan="2" style="vertical-align:middle;">
<p><span class="font3">WP218 &nbsp;&nbsp;&nbsp;</span><span class="font30" style="font-style:italic;">Use with page 4 of</span><span class="font30"> Piano, </span><span class="font30" style="font-style:italic;">Level 3.</span></p></td><td>
<p></p></td><td>
<p></p></td></tr>
</table>
<table border="1">
<tr><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">E</span></p></td><td>
<p><span class="font33">D</span></p></td><td>
<p><span class="font33">C</span></p></td></tr>
<tr><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p><span class="font33">F</span></p></td><td style="vertical-align:middle;">
<p><span class="font33">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font33">-</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">C</span></p></td><td>
<p><span class="font33">B</span></p></td><td>
<p><span class="font33">A</span></p></td><td>
<p><span class="font33">G</span></p></td><td>
<p><span class="font33">F</span></p></td><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font33">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font33">-</span></p></td></tr>
</table>
<p><span class="font33">Table 6.5: Actual pitch versus Calculated Pitch for figure A.3</span></p>
<p><span class="font33">1. In figure 6.3, 90.6% accuracy was obtained</span></p>
<p><span class="font33">2. In figure 6.4, 100% accuracy was obtained</span></p>
<p><span class="font33">3. In figure 6.5, 64.7% accuracy was obtained</span></p><h2><a name="bookmark80"></a><span class="font8">6.5 Note Duration</span></h2>
<p><span class="font33">The note duration for the score in figure A.2 obtained 100% accuracy but the scores in figures A.1 and A.3 did not perform well at all and this is due to the&nbsp;output from the level 2 segmentation module. We can therefore conclude that&nbsp;the level 2 segmentation module does not provide accurate results at present&nbsp;and should be improved in future releases.</span></p><h2><a name="bookmark81"></a><span class="font8">6.6 Neural Network</span></h2>
<p><span class="font33">The results of running the ‘testing' set as described in section 5.2.6 through the neural network are shown in table 6.6. An image of a sample glyph for each&nbsp;category is provided in appendix B.</span></p>
<table border="1">
<tr><td>
<p><span class="font33">Glyph</span></p>
<p><span class="font33">Name</span></p></td><td>
<p><span class="font33">Total</span></p>
<p><span class="font33">Glyphs</span></p></td><td>
<p><span class="font33">Average</span></p>
<p><span class="font33">Accuracy</span></p></td><td>
<p><span class="font33">Average</span></p>
<p><span class="font33">Confidence</span></p></td></tr>
<tr><td>
<p><span class="font33">bass</span></p></td><td>
<p><span class="font33">5</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">61.58%</span></p></td></tr>
<tr><td>
<p><span class="font33">crotchet</span></p></td><td>
<p><span class="font33">42</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">84.19%</span></p></td></tr>
<tr><td>
<p><span class="font33">demisemiquaver line</span></p></td><td>
<p><span class="font33">24</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">99.55%</span></p></td></tr>
<tr><td>
<p><span class="font33">flat</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">10</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">96.89%</span></p></td></tr>
<tr><td>
<p><span class="font33">minim</span></p></td><td>
<p><span class="font33">38</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">93.89%</span></p></td></tr>
<tr><td>
<p><span class="font33">natural</span></p></td><td>
<p><span class="font33">17</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">93.02%</span></p></td></tr>
<tr><td>
<p><span class="font33">quaver  br</span></p></td><td>
<p><span class="font33">5</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">99.51%</span></p></td></tr>
<tr><td>
<p><span class="font33">quaver line</span></p></td><td>
<p><span class="font33">3</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">99.51%</span></p></td></tr>
<tr><td>
<p><span class="font33">quaverjr</span></p></td><td>
<p><span class="font33">3</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">91.82%</span></p></td></tr>
<tr><td>
<p><span class="font33">semibreve</span></p></td><td>
<p><span class="font33">3</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">75.47%</span></p></td></tr>
<tr><td>
<p><span class="font33">semiquaver br</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">1</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">19.80%</span></p></td></tr>
<tr><td>
<p><span class="font33">semiquaver Jilie</span></p></td><td>
<p><span class="font33">55</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">94.87%</span></p></td></tr>
<tr><td>
<p><span class="font33">semiquaver tr</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33">1</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">71.13%</span></p></td></tr>
<tr><td>
<p><span class="font33">sharp</span></p></td><td>
<p><span class="font33">76</span></p></td><td>
<p><span class="font33">92.11%</span></p></td><td>
<p><span class="font33">84.95%</span></p></td></tr>
<tr><td>
<p><span class="font33">treble</span></p></td><td style="vertical-align:middle;">
<p><span class="font33">10</span></p></td><td>
<p><span class="font33">100%</span></p></td><td>
<p><span class="font33">99.60%</span></p></td></tr>
</table>
<p><span class="font33">Table 6.6: Neural Network Results</span></p>
<p><span class="font33">Out of the 15 different classes of glyphs trained by the neural network, the only class of data that did not obtain 100% accuracy was the ‘Sharp'. Neural&nbsp;networks are trained to remember certain patterns and in the case of the sharp&nbsp;its shape is similar to the natural glpyh which therefore explains why it is&nbsp;sometimes classyfing it as a natural.</span></p><h2><a name="bookmark82"></a><span class="font8">6.7 Monkey Testing</span></h2>
<p><span class="font33">The feedback provided by the two students was positive in the sense that the only way they managed to crash the application was to input string values in&nbsp;various input boxes. This is currently being fixed and each input box will be&nbsp;verified before accepting the input.</span></p><h2><a name="bookmark92"></a><span class="font8">6.8 Stress Testing</span></h2>
<p><span class="font33">When large images (greater than 5Mb) are used, the displaying of the score becomes extremely slow. However the system is still able to recognise the score.</span></p><h1><span class="font13">Chapter 7</span></h1><h1><a name="bookmark93"></a><span class="font16">Future Work</span></h1>
<p><span class="font33">As one of the major goals of this pro ject was to turn the OMR application developed into an open source project, it was important to identify aspects of&nbsp;the application that could be improved. Below is a set of suggested changes&nbsp;or enhancements that could be made to the current implementation and each&nbsp;suggestion is listed as having a low, medium, or critical priority.</span></p>
<p><span class="font33">1. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Improved Graphical User Interface (medium) </span><span class="font33">- Although much</span></p>
<p><span class="font33">time was spent developing the current GUI, much improvement can be</span></p>
<p><span class="font33">brought to it. The GUI was programmed in Swing and it may be worth investigating if it would be suitable to use SWT instead. Providing a&nbsp;zooming feature for the different images displayed is one example of a&nbsp;GUI enhancement.</span></p>
<p><span class="font33">2. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Interactive Features (low) </span><span class="font33">- A nice feature would be to allow users</span></p>
<p><span class="font33">to click on the recognised score in order to modify incorrectly identified notes. For example, if a note was omitted, the user could simply click on&nbsp;that note and the application would add it. Another interactive feature&nbsp;would be to illuminate the notes in a different colour as they are played.</span></p>
<p><span class="font33">3. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Level 2 Segmentation (high) </span><span class="font33">- The level 2 segmentation needs to&nbsp;be improved as this is currently affecting the overall performance of the&nbsp;application.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Multi-Threading (medium) </span><span class="font33">- The current version of the GUI is not</span></p>
<p><span class="font33">multi-threaded. This is only a minor issue which is unlikely to affect the execution time when recognising a score but would enhance the user's&nbsp;experience of the application.</span></p>
<p><span class="font33">5. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Detect Minims and Semibreves(critical) </span><span class="font33">- It is possible to detect</span></p>
<p><span class="font33">filled note heads by relying on the y-projection of the music score as described in SECTION. It is however very difficult to identify semibreves and</span></p>
<p><span class="font33">minims by using the current method used to locate note heads. A method</span></p>
<p><span class="font33">technique to detect minims and semibreves needs to be implemented.</span></p>
<p><span class="font33">6. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Support for Grand Staves (medium) </span><span class="font33">- Grand stave are typically&nbsp;used in piano scores where one stave is used for the right hand and the&nbsp;one below is used for the left hand. They are usually attached with vertical&nbsp;bars which is a feature that could be used to detect them. This feature&nbsp;could simply be implemented by having the user specify whether the score&nbsp;being recognised has a grand stave and if it is the case, two staves are&nbsp;processed at once when generating the midi file.</span></p>
<p><span class="font33">7. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Detect chords (critical) </span><span class="font33">- Chords are presently not implemented and in&nbsp;order for this application to support polyphonic scores, this feature needs&nbsp;to be implemented.</span></p>
<p><span class="font33">8. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Use Bass, Treble and Alto Clefs (critical) </span><span class="font33">- Although those clefs&nbsp;may be recognised and correctly classified at present, they do not affect&nbsp;the way in which the application generates the midi score and the default&nbsp;is the Treble clef.</span></p>
<p><span class="font33">9. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Outputting to LilyPond and MusicXML Formats (low) </span><span class="font33">- This&nbsp;would involve taking the internal representation of the recognised score&nbsp;and representing that the LilyPond or MusicXML formats. This is where&nbsp;perhaps a plug-in manager could be developed so that other output formats can be added at a later stage if necessary. Creating a LilyPond or&nbsp;MusicXML format only would a parser which knows the LilyPond and&nbsp;MusicXML formats.</span></p>
<p><span class="font33">10. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Saving Generated Midi Files (low) </span><span class="font33">- This extended feature could be&nbsp;provided as an extension to the openomr.midi package described in section&nbsp;4.1.6 and would enable users to save generated midi files to disk.</span></p>
<p><span class="font33">11. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Verifying correct length of measure (medium) </span><span class="font33">- If we are able to&nbsp;correctly identify the start and end of each measure in the music partition,&nbsp;we could add a module that checks the duration of the notes found in a&nbsp;measure add up correctly. For example, if we are in common time and our&nbsp;system detected five crotchets in a measure, we would know that an error&nbsp;occurred somewhere. We could then perhaps re-analysing that measure&nbsp;by varying certain parameters.</span></p>
<p><span class="font33">12. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Windowed FFT (medium) </span><span class="font33">- We saw in section 6.1 that when only a&nbsp;portion of the image has staves rotated, the FFT module performs poorly.&nbsp;The FFT module could be changed in such a way that smaller portions of&nbsp;the image are analysed.</span></p>
<p><span class="font33">13. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Semi-Automatic Neural Network Trainer (low) </span><span class="font33">- This feature&nbsp;would enhance the training process and overall accuracy of the neural&nbsp;network. We could have the program save all level 2 segments to disk and&nbsp;then sort the files according to the glyph types. This would avoid having&nbsp;to cut out glyphs in a an image editor which takes a lot of time.</span></p><h1><span class="font13">Chapter 8</span></h1><h1><a name="bookmark94"></a><span class="font16">Conclusion</span></h1>
<p><span class="font33">Optical music recognition (OMR) is the process of converting printed music scores into a format understandable by computers. Although several commercial&nbsp;applications exist, we have seen that they don't always perform accurately and&nbsp;this was the motivation for this project. We investigated the research that has&nbsp;been conducted in the past in order to gain a better understanding in this field.&nbsp;A segmentation based approach was used for the design of this project and was&nbsp;based on the O </span><span class="font29"><sup>3</sup> </span><span class="font33">MR project that was discussed in section 2.8.6.</span></p>
<p><span class="font33">The first step in a OMR application is to detect the thickness of stave lines and the spacing between the stave lines. We achieved this by using the RLE&nbsp;algorithm in order to produce a histogram for consecutive black and white pixel&nbsp;runs. The next phase involved detecting the stave lines and we saw that by&nbsp;taking the y-projection of the image, we were able locate the stave by looking&nbsp;for five equidistant peaks in the projection. When the staves were skewed, the&nbsp;stave detection algorithm did not perform well and a method to detect the&nbsp;angle by which the staves were rotated was used. This method involves taking&nbsp;the FFT of the image. We then proceeded to the level 0 segmentation process&nbsp;and found symbols that were close to one another. The note heads were then&nbsp;detected by means of the RLE algorithm and the y-projection. The level 1&nbsp;segmentation phase was then performed and this horizontally segmented level&nbsp;0 segments which contained note heads. The last segmentation phase was level&nbsp;2 and this involved vertically segmenting the image to produce glyphs. The&nbsp;glyphs were then input into the neural network and the outputs where used to&nbsp;reconstruct the score.</span></p>
<p><span class="font33">The results showed that the note head detection module of the OpenOMR application performed extremely well but the overall accuracy of the system&nbsp;was degraded by the level 2 segmentation module. The level 2 segmentation&nbsp;is a critical component of this system and is listed as having a critical priority&nbsp;for any future developments of this application. We also identified some areas&nbsp;which need improvement in chapter 7.</span></p><h1><span class="font13">Appendix A</span></h1><h1><a name="bookmark95"></a><span class="font16">Scores</span></h1>
<p><span class="font33">This appendix provides several scores that were used for the testing phase of the OpenOMR application.</span></p>
<p><span class="font28" style="font-weight:bold;">7</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-35.jpg" style="width:218pt;height:295pt;"/>
<p><span class="font33">Figure A.1: Testing Score Number 1</span></p>
<p><span class="font26" style="font-weight:bold;">A *C* scale in LilyPond</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font13" style="font-style:italic;">-y-</span></p></td><td>
<p><span class="font11">—1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font11">—rTir</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">/ * &nbsp;&nbsp;&nbsp;I</span></p></td><td>
<p><span class="font11"> 1 &gt;</span></p></td><td>
<p><span class="font11">L </span></p></td><td style="vertical-align:bottom;">
<p><span class="font11">1</span></p></td></tr>
<tr><td>
<p></p></td><td colspan="2">
<p><span class="font5" style="font-weight:bold;">&lt; r i r &gt; </span></p></td><td>
<p></p></td></tr>
<tr><td>
<p></p></td><td>
<p></p></td><td>
<p><span class="font11">'1</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">’ JJ</span></p></td></tr>
</table>
<p><span class="font33">Figure A.2: Testing Score Number 2</span></p>
<table border="1">
<tr><td>
<p><span class="font2">5 &nbsp;&nbsp;&nbsp;-</span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p><span class="font2">5 &nbsp;&nbsp;&nbsp;—-----</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">F- '■</span></p>
<p><span class="font1" style="font-style:italic;">1</span><span class="font0"> -M; /■«-^—---</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">--1----</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-f——X--</span><span class="font33" style="font-weight:bold;">C5 </span><span class="font2">4--</span></p></td><td style="vertical-align:bottom;">
<p><span class="font33" style="font-weight:bold;">--* dl J j </span><span class="font2">1-</span></p>
<p><span class="font33" style="font-weight:bold;">ff jq .. LLirq</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">—E!---</span></p></td></tr>
<tr><td>
<p><span class="font2">(=^4=^</span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p><span class="font33" style="font-weight:bold;">;^Eik=f= :</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;font-style:italic;">(d-</span><span class="font3" style="font-weight:bold;"> r^T'c-Tm</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">r^TTr^</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">H-</span><span class="font3">=-i</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font3" style="font-weight:bold;">........ &nbsp;&nbsp;&nbsp;i»&quot;&nbsp;&nbsp;&nbsp;&nbsp;'</span></p></td><td>
<p><span class="font3" style="font-weight:bold;">r </span><span class="font8" style="font-weight:bold;">F &gt; kJ</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">»-</span></p></td></tr>
<tr><td>
<p><span class="font3" style="font-weight:bold;">-C4%, &nbsp;&nbsp;&nbsp;</span><span class="font3">-I t t&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3" style="font-weight:bold;">•. 4 ™</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">-p-£---</span></p></td><td>
<p><span class="font3">—1—r </span><span class="font3" style="font-weight:bold;">r P*a j</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font3">1 VLJ &nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3" style="font-weight:bold;">.&nbsp;&nbsp;&nbsp;&nbsp;* 4</span></p></td><td>
<p><span class="font3" style="font-weight:bold;">*</span></p></td><td>
<p></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font3">| «J </span><span class="font3" style="font-weight:bold;">***'■ M</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">1</span></p>
<p><span class="font8" style="font-weight:bold;">rm n</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">......i</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font3" style="font-weight:bold;">F . &nbsp;&nbsp;&nbsp;</span><span class="font3">,.|, 1— .&nbsp;&nbsp;&nbsp;&nbsp;...&nbsp;&nbsp;&nbsp;&nbsp;. ... </span></p></td><td>
<p><span class="font3">————— —— -</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">—--———r</span></p></td><td>
<p></p></td></tr>
<tr><td>
<p><span class="font2" style="font-weight:bold;font-style:italic;">-J'</span><span class="font3"> &nbsp;&nbsp;&nbsp;--J </span><span class="font3" style="font-weight:bold;">W « - - -,.-4 - .</span></p></td><td>
<p><span class="font3">1 1 ^ir t &nbsp;&nbsp;&nbsp;</span><span class="font3" style="font-weight:bold;">J</span></p></td><td>
<p></p></td><td>
<p><span class="font6" style="font-weight:bold;font-variant:small-caps;">- -  j .. &nbsp;&nbsp;&nbsp;</span><span class="font3">j</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font8" style="font-weight:bold;">l &gt; * « </span><span class="font2" style="font-weight:bold;font-style:italic;">a &nbsp;&nbsp;&nbsp;</span><span class="font6" style="font-weight:bold;font-variant:small-caps;">j</span></p></td><td>
<p><span class="font2" style="font-weight:bold;font-style:italic;">a * a</span><span class="font3"> &nbsp;&nbsp;&nbsp;1 </span><span class="font30" style="font-weight:bold;font-variant:small-caps;">f</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">“ &nbsp;&nbsp;&nbsp;+-<sub>2</sub>&nbsp;&nbsp;&nbsp;&nbsp;j&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;&nbsp;&nbsp;&nbsp;-</span></p></td><td>
<p><span class="font8" style="font-weight:bold;">J-<sup>1</sup></span></p></td><td>
<p><span class="font8" style="font-weight:bold;">J-J^-^1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">F-~ J J-.</span></p></td></tr>
</table>
<table border="1">
<tr><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font9">y-</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font9">JTn£^,</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font9">-H;-F=—F--F=-H </span></p></td><td>
<p><span class="font9">J -&gt; *</span></p>
<p><span class="font9">—F---p——</span></p></td><td style="vertical-align:bottom;">
<p><span class="font9">-J J J-</span></p>
<p><span class="font9">=pr-=r---</span></p></td><td style="vertical-align:bottom;">
<p><span class="font9">—1--T</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font9"><sup>1</sup></span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td></tr>
</table>
<table border="1">
<tr><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font29"><sup>1 4</sup> ----</span></p>
<p><span class="font29">i-.......</span></p>
<p><span class="font29">TO--</span></p></td><td style="vertical-align:bottom;">
<p><span class="font29">—F——F---</span></p></td><td>
<p><span class="font29">3</span></p>
<p><span class="font29">/</span></p></td></tr>
<tr><td>
<p><span class="font37" style="font-variant:small-caps;"><sup>1L/</sup> ~4l3*</span><span class="font29"> -=j</span></p>
<p><span class="font29"><sup>5</sup> 2 1</span></p></td><td>
<p><span class="font4" style="font-weight:bold;">. pr. * « -:—</span></p></td><td style="vertical-align:bottom;">
<p><span class="font29">fi</span></p></td><td>
<p></p></td></tr>
</table>
<p><span class="font2" style="font-weight:bold;">WP218 </span><span class="font29" style="font-style:italic;">Use with page 4 of</span><span class="font29"> Piano, </span><span class="font29" style="font-style:italic;">Level 3.</span></p>
<p><span class="font33">Figure A.3: Testing Score Number 3</span></p><h1><span class="font13">Appendix B</span></h1><h1><a name="bookmark96"></a><span class="font16">Musical Glyphs Used</span></h1>
<p><span class="font33">This appendix provides all the glyphs that were used to train the neural network.</span></p><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-36.png" style="width:25pt;height:17pt;"/></div><br clear="all"/><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-37.png" style="width:21pt;height:50pt;"/></div><br clear="all"/><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-38.png" style="width:39pt;height:23pt;"/></div><br clear="all"/><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-39.jpg" style="width:23pt;height:52pt;"/></div><br clear="all"/><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-40.jpg" style="width:27pt;height:24pt;"/></div><br clear="all"/><div><h1><a name="bookmark97"></a><span class="font37">V]</span></h1></div><br clear="all"/><div>
<p><span class="font21" style="font-weight:bold;text-decoration:underline;">=5</span></p></div><br clear="all"/><div><img src="MSc Dissertation Report_files/MSc Dissertation Report-41.jpg" style="width:36pt;height:88pt;"/></div><br clear="all"/>
<p><span class="font33">Table B.1: FFT Module Results</span></p><h1><span class="font13">Appendix C</span></h1><h1><a name="bookmark98"></a><span class="font16">OpemOMR User Guide</span></h1>
<p><span class="font33">This appendix provides a user guide along with screen shots for the OpenOMR application.</span></p>
<p><span class="font33">1. </span><span class="font33" style="font-weight:bold;">Opening an image: </span><span class="font33">The first step to recognise a scanned music score. You can open a music score by either clicking on the ‘open folder' in the</span></p>
<p><span class="font33">toolbar or by going to the “File - Open” menu. You will then be prompted to select a file. Upon selecting a file, the image of the score opened will&nbsp;be displayed in the main window. You should now have a screen as shown&nbsp;below:</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-42.jpg" style="width:81pt;height:96pt;"/>
<p><span class="font33">2. </span><span class="font33" style="font-weight:bold;">Recognising a score: </span><span class="font33">The next step is to recognise the score and this&nbsp;can be done by clicking on the “green flag” button in the toolbar or going&nbsp;to the “Recognition - Recognise” menu. You will then be prompted with&nbsp;a box as shown below:</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-43.jpg" style="width:126pt;height:126pt;"/>
<p><span class="font33">You should first try recognising the score with the default parameters. If once the recognition is over and you find that the wrong number of staves&nbsp;were found, you may adjust the parameters in the input boxes and try the&nbsp;recognition process.</span></p>
<p><span class="font33">To view the recognised score, go to the “Recognise - View Recognised Score” menu. The recognised score will then be displayed as shown below:</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-44.jpg" style="width:228pt;height:143pt;"/>
<p><span class="font33">3. </span><span class="font33" style="font-weight:bold;">Skewed staves: </span><span class="font33">If you are under the impression that the staves are&nbsp;skewed, you can de-skewing it by pressing the “FFT” icon on the toolbar&nbsp;or by going to the “FFT - FFT” menu. A window will be displayed as&nbsp;shown below:</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-45.jpg" style="width:147pt;height:150pt;"/>
<p><span class="font33">The window will prompt you for the window size of the FFT and the input must be a power of 2. That is, is can be 512, 1024, 2048, etc. Now click&nbsp;on the “Do FFT” button. The application will try to determine the angle&nbsp;by which the staves are rotated. When the window disappears, you can&nbsp;then proceed to the recognition process of the score.</span></p>
<p><span class="font33">4. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Playing the score: </span><span class="font33">You can play the recognised score by pressing the&nbsp;blue triangle in the toolbar menu.</span></p>
<p><span class="font33">5. &nbsp;&nbsp;&nbsp;</span><span class="font33" style="font-weight:bold;">Viewing graphs: </span><span class="font33">Various graphs of x and y-projections can be viewed&nbsp;through the “Analysis” menu. When selecting a graph to view, a new tab&nbsp;in the main window will be created as shown below:</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-46.jpg" style="width:228pt;height:143pt;"/>
<p><span class="font33">6. </span><span class="font33" style="font-weight:bold;">Training and testing the neural network: </span><span class="font33">To train the neural network, go to the “ANN - Trainer” menu. A new window will be displayed as shown below:</span></p><img src="MSc Dissertation Report_files/MSc Dissertation Report-47.jpg" style="width:197pt;height:100pt;"/>
<p><span class="font33">You must now choose the directory which contains the “training” and “validation” sub-directories in which contain the sample glyphs. Now click&nbsp;on the “Train” button. The neural network will now train and the training&nbsp;will stop when the validation RMSE starts to increase. If after a large&nbsp;amount of epochs (i.e 3,000) the neural network has not stopped training,&nbsp;you can manually stop the training by pressing the “Stop” button. You&nbsp;will now want to save the state of the trained neural network and this can&nbsp;be done by clicking the “Save” button.</span></p><h1><a name="bookmark99"></a><span class="font16">Bibliography</span></h1>
<p><span class="font33">[1] Bainbridge, D. (1996). Optical Music Recognition: A Generalised Approach&nbsp;</span><span class="font4">Department of Computer Science, University of Canterbury</span><span class="font33">. Christchurch,&nbsp;New Zealand</span></p>
<p><span class="font33">[2] Bainbridge, D. (1997). An extensible optical music recognition system </span><span class="font4">Nineteenth Australasian Computer Science Conference</span><span class="font33">. Melbourne, Australia</span></p>
<p><span class="font33">[3] Bainbridge, D. (2005). Fast Capture of Sheet Music for an Agile Digital&nbsp;Music Library </span><span class="font4">Department of Computer Science, University of Canterbury</span><span class="font33">.&nbsp;Christchurch, New Zealand</span></p>
<p><span class="font33">[4] Bellini, P. (2001). Optical Music Sheet Segmentation </span><span class="font4">Proceedings of the First&nbsp;International Conference on WEB Delivering Music</span></p>
<p><span class="font33">[5] Bellini, P; Bruno I; Nessi, P. Assessing Optical Music Recognition Tools</span></p>
<p><span class="font33">[6] Bolstein, D. A Critical Survey of Music and Image Analysis </span><span class="font4">Department of&nbsp;Computing and Information Science, Queen's University</span><span class="font33">. Ontario, Canada</span></p>
<p><span class="font33">[7] George, S. (2005). Visual Perception of Music Notation: On-Line and OffLine Recognition </span><span class="font4">IRM Press. ISBN: 1-931777-94-2</span></p>
<p><span class="font33">[8] Ng, Kia; Barthelemy, J; Ong B; Bruno I; Nesi P. CIMS: Coding Images of&nbsp;Music Sheets </span><span class="font4">The Interactive-Music Network, 2005</span></p>
<p><span class="font33">[9] Roth, M. (1994). An Approach to Recognition of Printed Music </span><span class="font4">Swiss Federal&nbsp;Institute of Technology</span><span class="font33">. Zurich, Switzerland</span></p>
<p><span class="font33">[10] Sau Dan, L. Automatic Optical Music Recognition </span><span class="font4">Department of Computer Science, University of Hong Kong</span></p>
<p><span class="font33">[11] Colton, Simon: Introduction to Artificial Intelligence</span></p>
<p><span class="font33" style="font-style:italic;"><a href="http://www.doc.ic.ac.uk/~sgc/teaching/v231/index.html">http://www.doc.ic.ac.uk/</span><span class="font4" style="font-style:italic;">~</span><span class="font33" style="font-style:italic;">sgc/teaching/v231/index.html</a> Department of</span></p>
<p><span class="font33" style="font-style:italic;">Computing, Imperial College</span><span class="font33"> London, UK</span></p>
<p><span class="font33">[12] Johnston, Alex: Classifying Persian Characters with Artificial Neural Networks and Inverted Complex Zernike Moments </span><span class="font33" style="font-style:italic;">Department of Computing,&nbsp;Imperial College</span><span class="font33"> London, UK</span></p>
<p><span class="font33">[13] Joone - Java Object Oriented Neural Engine <a href="http://www.jooneworld.com">http://www.jooneworld.com</a></span></p>
<p><span class="font33" style="font-style:italic;">Joone reference manual</span></p>
<p><span class="font33">[14] <a href="http://icking-music-archive.org/software/indexmt6.html">http://icking-music-archive.org/software/indexmt6.html</a></span></p>
<p><span class="font33" style="font-style:italic;">MusiXT<sub>E</sub>Xreference manual</span></p>
<p><span class="font33">[15] <a href="http://www.lilypond.org">http://www.lilypond.org</a></span></p>
<p><span class="font33">[16] <a href="http://www.recordare.com/xml/faq.html">http://www.recordare.com/xml/faq.html</a></span></p>
<p><span class="font33">[17] <a href="http://www.wikipedia.org/wiki/NIFF">http://www.wikipedia.org/wiki/NIFF</a></span></p>
<p><span class="font33">[18] &nbsp;&nbsp;&nbsp;<a href="http://en.wikipedia.org/wiki/Musical_notation">http://en.wikipedia.org/wiki/Musical_notation</a></span></p>
<p><span class="font33">[19] &nbsp;&nbsp;&nbsp;<a href="http://www.cs.unm.edu/~brayer/vision/fourier.html">http://www.cs.unm.edu/</span><span class="font4">~</span><span class="font33">brayer/vision/fourier.html</a> </span><span class="font33" style="font-style:italic;">Introduction to the&nbsp;Fourier Transform</span></p>
<p><span class="font33">[20] &nbsp;&nbsp;&nbsp;Kerry Peake: Optical Music Recognition using Kd-tree Decomposition </span><span class="font33" style="font-style:italic;">University of Birmingham</span><span class="font33"> UK</span></p>
<p><span class="font33">[21] &nbsp;&nbsp;&nbsp;Yong Li: Music Sheet Reader - An Implementation of Optical Music Recognition System </span><span class="font33" style="font-style:italic;">University of Birmingham</span><span class="font33"> UK</span></p>
<p><span class="font33">[22] &nbsp;&nbsp;&nbsp;Peltarion:&nbsp;&nbsp;&nbsp;&nbsp;Tutorial II - Good Cop/Bad Cop</span></p>
<p><span class="font33"><a href="http://www.peltarion.com/WebDoc/Tutorials/tutorial2.html">http://www.peltarion.com/WebDoc/Tutorials/tutorial2.html</a></span></p>
<p><span class="font33">77</span></p>
<p><a name="bookmark84"><sup><a href="#footnote1">1</a></sup></a></p>
<p><span class="font33"> Pre-processing - This stage involves correcting skewed staves, removing&nbsp;the stave lines and segmenting the image. Realigning the score is accomplished by using a line-chasing algorithm to identify the top stave and&nbsp;consequently its skew which then allows the stave line to be realigned.</span></p>
<p><a name="bookmark85"><sup><a href="#footnote2">2</a></sup></a></p>
<p><span class="font33">The staves are then removed by using a line chasing algorithm and deletes</span></p>
<p><a name="bookmark86"><sup><a href="#footnote3">3</a></sup></a></p>
<p><span class="font33">black pixels except where an object is identified. The segmentation process is done by locating the boundary of each ob ject in the image.</span></p>
<p><a name="bookmark87"><sup><a href="#footnote4">4</a></sup></a></p>
<p><span class="font33"> Momentum is added to the weight changes. This approach involves remembering the change that was added to each weight in the previous&nbsp;epoch and adding a small amount of that weight to the weight in the current epoch. How much is added is controlled by the momentum parameter&nbsp;which ranges between 0 and -.</span></p>
</body>
</html>